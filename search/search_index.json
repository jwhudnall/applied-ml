{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Lessons All the Applied ML course lessons can be found here . Set up python3 -m venv venv source venv/bin/activate python -m pip install --upgrade pip make install-dev Start Jupyterlab python -m ipykernel install --user --name = tagifai jupyter labextension install @jupyter-widgets/jupyterlab-manager jupyter labextension install @jupyterlab/toc jupyter lab You can also run all notebooks on Google Colab . Directory structure tagifai/ \u251c\u2500\u2500 config.py - configuration setup \u251c\u2500\u2500 data.py - data processing utilities \u251c\u2500\u2500 main.py - main operations (CLI) \u251c\u2500\u2500 models.py - model architectures \u251c\u2500\u2500 predict.py - inference utilities \u251c\u2500\u2500 train.py - training utilities \u2514\u2500\u2500 utils.py - supplementary utilities Documentation can be found here . MLFlow mlflow server -h 0.0.0.0 -p 5000 --backend-store-uri assets/experiments/ Mkdocs python -m mkdocs serve","title":"TagIfAI"},{"location":"#lessons","text":"All the Applied ML course lessons can be found here .","title":"Lessons"},{"location":"#set-up","text":"python3 -m venv venv source venv/bin/activate python -m pip install --upgrade pip make install-dev","title":"Set up"},{"location":"#start-jupyterlab","text":"python -m ipykernel install --user --name = tagifai jupyter labextension install @jupyter-widgets/jupyterlab-manager jupyter labextension install @jupyterlab/toc jupyter lab You can also run all notebooks on Google Colab .","title":"Start Jupyterlab"},{"location":"#directory-structure","text":"tagifai/ \u251c\u2500\u2500 config.py - configuration setup \u251c\u2500\u2500 data.py - data processing utilities \u251c\u2500\u2500 main.py - main operations (CLI) \u251c\u2500\u2500 models.py - model architectures \u251c\u2500\u2500 predict.py - inference utilities \u251c\u2500\u2500 train.py - training utilities \u2514\u2500\u2500 utils.py - supplementary utilities Documentation can be found here .","title":"Directory structure"},{"location":"#mlflow","text":"mlflow server -h 0.0.0.0 -p 5000 --backend-store-uri assets/experiments/","title":"MLFlow"},{"location":"#mkdocs","text":"python -m mkdocs serve","title":"Mkdocs"},{"location":"api/","text":"This content will be coming soon once we cover the API lesson! In the meantime: subscribe to our monthly newsletter follow us on Twitter and LinkedIn for updates and tips.","title":"API"},{"location":"workflows/","text":"Workflow View all available options using the CLI app: # View all CLI commands $ tagifai --help Usage: tagifai [OPTIONS] COMMAND [ARGS] \ud83d\udc49 Commands: download-data Download data from online to local drive. optimize Optimize a subset of hyperparameters towards ... train-model Predict tags for a give input text using a ... predict-tags Train a model using the specified parameters. # Help for a specific command $ tagifai train-model --help Usage: tagifai train-model [OPTIONS] Options: --args-fp PATH [default: config/args.json] --help Show this message and exit. # Train a model $ tagifai train-model --args-fp $PATH \ud83d\ude80 Training... Download the necessary data files to assets/data . tagifai download-data Optimize using distributions specified in tagifai.train.objective . This also writes the best model's args to config/args.json tagifai optimize --num-trials 100 We'll cover how to train using compute instances on the cloud from Amazon Web Services (AWS) or Google Cloud Platforms (GCP) in later lessons. But in the meantime, if you don't have access to GPUs, check out the optimize.ipynb notebook for how to train on Colab and transfer to local. We essentially run optimization, then train the best model to download and transfer it's arguments and artifacts. Once we have them in our local machine, we can run tagifai set-artifact-metadata to match all metadata as if it were run from your machine. Train a model (and save all it's artifacts) using args from config/args.json tagifai train-model --args-fp config/args.json Predict tags for an input sentence. It'll use the best model saved from train-model but you can also specify a run-id to choose a specific model. tagifai predict-tags --text \"Transfer learning with BERT\"","title":"Workflow"},{"location":"workflows/#workflow","text":"View all available options using the CLI app: # View all CLI commands $ tagifai --help Usage: tagifai [OPTIONS] COMMAND [ARGS] \ud83d\udc49 Commands: download-data Download data from online to local drive. optimize Optimize a subset of hyperparameters towards ... train-model Predict tags for a give input text using a ... predict-tags Train a model using the specified parameters. # Help for a specific command $ tagifai train-model --help Usage: tagifai train-model [OPTIONS] Options: --args-fp PATH [default: config/args.json] --help Show this message and exit. # Train a model $ tagifai train-model --args-fp $PATH \ud83d\ude80 Training... Download the necessary data files to assets/data . tagifai download-data Optimize using distributions specified in tagifai.train.objective . This also writes the best model's args to config/args.json tagifai optimize --num-trials 100 We'll cover how to train using compute instances on the cloud from Amazon Web Services (AWS) or Google Cloud Platforms (GCP) in later lessons. But in the meantime, if you don't have access to GPUs, check out the optimize.ipynb notebook for how to train on Colab and transfer to local. We essentially run optimization, then train the best model to download and transfer it's arguments and artifacts. Once we have them in our local machine, we can run tagifai set-artifact-metadata to match all metadata as if it were run from your machine. Train a model (and save all it's artifacts) using args from config/args.json tagifai train-model --args-fp config/args.json Predict tags for an input sentence. It'll use the best model saved from train-model but you can also specify a run-id to choose a specific model. tagifai predict-tags --text \"Transfer learning with BERT\"","title":"Workflow"},{"location":"tagifai/config/","text":"In this file we're setting up the configuration needed for all our workflows. First up is creating required directories so we can save and load from them: # Directories BASE_DIR = Path ( __file__ ) . parent . parent . absolute () CONFIG_DIR = Path ( BASE_DIR , \"config\" ) LOGS_DIR = Path ( BASE_DIR , \"logs\" ) ASSETS_DIR = Path ( BASE_DIR , \"assets\" ) DATA_DIR = Path ( ASSETS_DIR , \"data\" ) EXPERIMENTS_DIR = Path ( ASSETS_DIR , \"experiments\" ) # Create dirs utils . create_dirs ( dirpath = LOGS_DIR ) utils . create_dirs ( dirpath = ASSETS_DIR ) utils . create_dirs ( dirpath = DATA_DIR ) utils . create_dirs ( dirpath = EXPERIMENTS_DIR ) Then we'll establish our logger using the configuration file from config/logging.config : # Loggers logging . config . fileConfig ( Path ( CONFIG_DIR , \"logging.config\" )) logger = logging . getLogger () logger . handlers [ 0 ] = RichHandler ( markup = True ) # set rich handler Finally we'll set the tracking URI for all MLFlow experiments: # MLFlow mlflow . set_tracking_uri ( \"file://\" + str ( EXPERIMENTS_DIR . absolute ()))","title":"Configuration"},{"location":"tagifai/config/#tagifai.config","text":"","title":"tagifai.config"},{"location":"tagifai/data/","text":"Classes CNNTextDataset Create torch.utils.data.Dataset objects to use for efficiently feeding data into our models. Usage: # Create dataset X , y = data dataset = CNNTextDataset ( X = X , y = y , max_filter_size = max_filter_size ) # Create dataloaders dataloader = dataset . create_dataloader ( batch_size = batch_size ) Methods collate_fn ( self , batch ) Processing on a batch. It's used to override the default collate_fn in torch.utils.data.DataLoader . Parameters: Name Type Description Default batch List List of inputs and outputs. required Returns: Type Description Tuple Processed inputs and outputs. Source code in tagifai/data.py def collate_fn ( self , batch : List ) -> Tuple : \"\"\"Processing on a batch. It's used to override the default `collate_fn` in `torch.utils.data.DataLoader`. Args: batch (List): List of inputs and outputs. Returns: Processed inputs and outputs. \"\"\" # Get inputs X = np . array ( batch , dtype = object )[:, 0 ] y = np . stack ( np . array ( batch , dtype = object )[:, 1 ], axis = 0 ) # Pad inputs X = pad_sequences ( sequences = X , max_seq_len = self . max_filter_size ) # Cast X = torch . LongTensor ( X . astype ( np . int32 )) y = torch . FloatTensor ( y . astype ( np . int32 )) return X , y create_dataloader ( self , batch_size , shuffle = False , drop_last = False ) Create dataloaders to load batches with. Usage: # Create dataset X , y = data dataset = CNNTextDataset ( X = X , y = y , max_filter_size = max_filter_size ) # Create dataloaders dataloader = dataset . create_dataloader ( batch_size = batch_size ) Parameters: Name Type Description Default batch_size int Number of samples per batch. required shuffle bool Shuffle each batch. Defaults to False. False drop_last bool Drop the last batch if it's less than batch_size . Defaults to False. False Returns: Type Description torch.utils.data.dataloader.DataLoader Torch dataloader to load batches with. Source code in tagifai/data.py def create_dataloader ( self , batch_size : int , shuffle : bool = False , drop_last : bool = False ) -> torch . utils . data . DataLoader : \"\"\"Create dataloaders to load batches with. Usage: ```python # Create dataset X, y = data dataset = CNNTextDataset(X=X, y=y, max_filter_size=max_filter_size) # Create dataloaders dataloader = dataset.create_dataloader(batch_size=batch_size) ``` Args: batch_size (int): Number of samples per batch. shuffle (bool, optional): Shuffle each batch. Defaults to False. drop_last (bool, optional): Drop the last batch if it's less than `batch_size`. Defaults to False. Returns: Torch dataloader to load batches with. \"\"\" return torch . utils . data . DataLoader ( dataset = self , batch_size = batch_size , collate_fn = self . collate_fn , shuffle = shuffle , drop_last = drop_last , pin_memory = True , ) LabelEncoder Encode labels into unique indices. Usage: # Encode labels label_encoder = LabelEncoder () label_encoder . fit ( labels ) y = label_encoder . encode ( labels ) Methods decode ( self , y ) Decode a (multilabel) one-hot encoding into corresponding labels. Parameters: Name Type Description Default y ndarray Labels as (multilabel) one-hot encodings required Returns: Type Description List[List[str]] List of original labels for each output. Source code in tagifai/data.py def decode ( self , y : np . ndarray ) -> List [ List [ str ]]: \"\"\"Decode a (multilabel) one-hot encoding into corresponding labels. Args: y (np.ndarray): Labels as (multilabel) one-hot encodings Returns: List of original labels for each output. \"\"\" classes = [] for i , item in enumerate ( y ): indices = np . where ( item == 1 )[ 0 ] classes . append ([ self . index_to_class [ index ] for index in indices ]) return classes encode ( self , y ) Encode a collection of labels using (multilabel) one-hot encoding. Parameters: Name Type Description Default y Series Collection of labels as a pandas Series object. required Returns: Type Description ndarray Labels as (multilabel) one-hot encodings Source code in tagifai/data.py def encode ( self , y : pd . Series ) -> np . ndarray : \"\"\"Encode a collection of labels using (multilabel) one-hot encoding. Args: y (pd.Series): Collection of labels as a pandas Series object. Returns: Labels as (multilabel) one-hot encodings \"\"\" y_one_hot = np . zeros (( len ( y ), len ( self . class_to_index )), dtype = int ) for i , item in enumerate ( y ): for class_ in item : y_one_hot [ i ][ self . class_to_index [ class_ ]] = 1 return y_one_hot fit ( self , y ) Learn label mappings from a series of class labels. Parameters: Name Type Description Default y Sequence Collection of labels as a pandas Series object. required Source code in tagifai/data.py def fit ( self , y : Sequence ): \"\"\"Learn label mappings from a series of class labels. Args: y (Sequence): Collection of labels as a pandas Series object. \"\"\" classes = np . unique ( list ( itertools . chain . from_iterable ( y ))) for i , class_ in enumerate ( classes ): self . class_to_index [ class_ ] = i self . index_to_class = { v : k for k , v in self . class_to_index . items ()} self . classes = list ( self . class_to_index . keys ()) return self Tokenizer Tokenize a feature using a built vocabulary. Usage: tokenizer = Tokenizer ( char_level = char_level ) tokenizer . fit_on_texts ( texts = X ) X = np . array ( tokenizer . texts_to_sequences ( X )) Methods fit_on_texts ( self , texts ) Learn token mappings from a list of texts. Parameters: Name Type Description Default texts List List of texts made of tokens. required Source code in tagifai/data.py def fit_on_texts ( self , texts : List ): \"\"\"Learn token mappings from a list of texts. Args: texts (List): List of texts made of tokens. \"\"\" if self . char_level : all_tokens = [ token for text in texts for token in text ] if not self . char_level : all_tokens = [ token for text in texts for token in text . split ( \" \" )] counts = Counter ( all_tokens ) . most_common ( self . num_tokens ) self . min_token_freq = counts [ - 1 ][ 1 ] for token , count in counts : index = len ( self ) self . token_to_index [ token ] = index self . index_to_token [ index ] = token return self sequences_to_texts ( self , sequences ) Convert a lists of arrays of indices to a list of texts. Parameters: Name Type Description Default sequences List list of mapped tokens to convert back to text. required Returns: Type Description List Mapped text from index tokens. Source code in tagifai/data.py def sequences_to_texts ( self , sequences : List ) -> List : \"\"\"Convert a lists of arrays of indices to a list of texts. Args: sequences (List): list of mapped tokens to convert back to text. Returns: Mapped text from index tokens. \"\"\" texts = [] for sequence in sequences : text = [] for index in sequence : text . append ( self . index_to_token . get ( index , self . oov_token )) texts . append ( self . separator . join ([ token for token in text ])) return texts texts_to_sequences ( self , texts ) Convert a list of texts to a lists of arrays of indices. Parameters: Name Type Description Default texts List List of texts to tokenize and map to indices. required Returns: Type Description List[numpy.ndarray] A list of mapped tokens. Source code in tagifai/data.py def texts_to_sequences ( self , texts : List ) -> List [ np . ndarray ]: \"\"\"Convert a list of texts to a lists of arrays of indices. Args: texts (List): List of texts to tokenize and map to indices. Returns: A list of mapped tokens. \"\"\" sequences = [] for text in texts : if not self . char_level : text = text . split ( \" \" ) sequence = [] for token in text : sequence . append ( self . token_to_index . get ( token , self . token_to_index [ self . oov_token ] ) ) sequences . append ( np . asarray ( sequence )) return sequences Functions clean ( df , tags_dict , min_tag_freq = 30 ) Cleaning the raw data. Parameters: Name Type Description Default df DataFrame Pandas DataFrame with data. required tags_dict Dict Dictionary of tags and their metadata (parents, aliases, etc.) required min_tag_freq int Minimum frequency of tags required. Defaults to 30. 30 Returns: Type Description Tuple A cleaned dataframe, dictionary of all tags and tags above the frequency threshold. Source code in tagifai/data.py def clean ( df : pd . DataFrame , tags_dict : Dict , min_tag_freq : int = 30 ) -> Tuple : \"\"\"Cleaning the raw data. Args: df (pd.DataFrame): Pandas DataFrame with data. tags_dict (Dict): Dictionary of tags and their metadata (parents, aliases, etc.) min_tag_freq (int, optional): Minimum frequency of tags required. Defaults to 30. Returns: A cleaned dataframe, dictionary of all tags and tags above the frequency threshold. \"\"\" # Combine features df [ \"text\" ] = df . title + \" \" + df . description # Inclusion/exclusion criteria for tags include = list ( tags_dict . keys ()) exclude = [ \"machine-learning\" , \"deep-learning\" , \"data-science\" , \"neural-networks\" , \"python\" , \"r\" , \"visualization\" , ] # Filter tags for each project df . tags = df . tags . apply ( filter_items , include = include , exclude = exclude ) tags = Counter ( itertools . chain . from_iterable ( df . tags . values )) # Filter tags that have fewer than `min_tag_freq` occurrences tags_above_freq = Counter ( tag for tag in tags . elements () if tags [ tag ] >= min_tag_freq ) df . tags = df . tags . apply ( filter_items , include = list ( tags_above_freq . keys ())) # Remove projects with no more remaining relevant tags df = df [ df . tags . map ( len ) > 0 ] return df , tags_dict , tags_above_freq encode_labels ( labels ) Encode labels into unique integers. Usage: y , class_weights , label_encoder = data . encode_labels ( labels = df . tags ) Parameters: Name Type Description Default labels Series Pandas Series of all the labels. required Returns: Type Description tuple Encoded labels, class weights and the encoder. Source code in tagifai/data.py def encode_labels ( labels : pd . Series ) -> tuple : \"\"\"Encode labels into unique integers. Usage: ```python y, class_weights, label_encoder = data.encode_labels(labels=df.tags) ``` Args: labels (pd.Series): Pandas Series of all the labels. Returns: Encoded labels, class weights and the encoder. \"\"\" # Encode labels label_encoder = LabelEncoder () label_encoder . fit ( labels ) y = label_encoder . encode ( labels ) # Class weights all_tags = list ( itertools . chain . from_iterable ( labels . values )) counts = np . bincount ( [ label_encoder . class_to_index [ class_ ] for class_ in all_tags ] ) class_weights = { i : 1.0 / count for i , count in enumerate ( counts )} return y , class_weights , label_encoder filter_items ( items , include = [], exclude = []) Filter a list using inclusion and exclusion lists of items. Parameters: Name Type Description Default items List List of items to apply filters. required include List List of items to include. Defaults to []. [] exclude List List of items to filter out. Defaults to []. [] Returns: Type Description List Filtered list of items. # Inclusion/exclusion criteria for tags include = list ( tags_dict . keys ()) exclude = [ \"machine-learning\" , \"deep-learning\" , \"data-science\" , \"neural-networks\" , \"python\" , \"r\" , \"visualization\" , ] # Filter tags for each project df . tags = df . tags . apply ( filter_items , include = include , exclude = exclude ) Source code in tagifai/data.py def filter_items ( items : List , include : List = [], exclude : List = []) -> List : \"\"\"Filter a list using inclusion and exclusion lists of items. Args: items (List): List of items to apply filters. include (List, optional): List of items to include. Defaults to []. exclude (List, optional): List of items to filter out. Defaults to []. Returns: Filtered list of items. Usage: ```python # Inclusion/exclusion criteria for tags include = list(tags_dict.keys()) exclude = [ \"machine-learning\", \"deep-learning\", \"data-science\", \"neural-networks\", \"python\", \"r\", \"visualization\", ] # Filter tags for each project df.tags = df.tags.apply(filter_items, include=include, exclude=exclude) ``` \"\"\" filtered = [ item for item in items if item in include and item not in exclude ] return filtered get_dataloader ( data , max_filter_size , batch_size ) Create dataloader from data. Parameters: Name Type Description Default data Tuple Data to load into the DataLoader object. required max_filter_size int length of the largest CNN filter. required batch_size int number of samples per batch. required Returns: Type Description Tuple Created dataloader with data. Source code in tagifai/data.py def get_dataloader ( data : Tuple , max_filter_size : int , batch_size : int ) -> Tuple : \"\"\"Create dataloader from data. Args: data (Tuple): Data to load into the DataLoader object. max_filter_size (int): length of the largest CNN filter. batch_size (int): number of samples per batch. Returns: Created dataloader with data. \"\"\" # Create dataset X , y = data dataset = CNNTextDataset ( X = X , y = y , max_filter_size = max_filter_size ) # Create dataloaders dataloader = dataset . create_dataloader ( batch_size = batch_size ) return dataloader iterative_train_test_split ( X , y , train_size = 0.7 ) Custom iterative train test split which 'maintains balanced representation with respect to order-th label combinations.' Parameters: Name Type Description Default X Series Input features as a pandas Series object. required y ndarray One-hot encoded labels. required train_size float Proportion of data for first split. Defaults to 0.7. 0.7 Returns: Type Description Tuple Two stratified splits based on specified proportions. Source code in tagifai/data.py def iterative_train_test_split ( X : pd . Series , y : np . ndarray , train_size : float = 0.7 ) -> Tuple : \"\"\"Custom iterative train test split which 'maintains balanced representation with respect to order-th label combinations.' Args: X (pd.Series): Input features as a pandas Series object. y (np.ndarray): One-hot encoded labels. train_size (float, optional): Proportion of data for first split. Defaults to 0.7. Returns: Two stratified splits based on specified proportions. \"\"\" stratifier = IterativeStratification ( n_splits = 2 , order = 1 , sample_distribution_per_fold = [ 1.0 - train_size , train_size , ], ) train_indices , test_indices = next ( stratifier . split ( X , y )) X_train , y_train = X [ train_indices ], y [ train_indices ] X_test , y_test = X [ test_indices ], y [ test_indices ] return X_train , X_test , y_train , y_test load ( shuffle , num_samples = 0 ) Load the data from local drive to a Pandas DataFrame. Parameters: Name Type Description Default shuffle bool Shuffle the data. required num_samples int Number of samples to include (used for quick testting). Defaults to 0 which includes all samples. 0 Returns: Type Description DataFrame Dataframe, projects and tags dictionaries. Source code in tagifai/data.py def load ( shuffle : bool , num_samples : int = 0 ) -> pd . DataFrame : \"\"\"Load the data from local drive to a Pandas DataFrame. Args: shuffle (bool): Shuffle the data. num_samples (int, optional): Number of samples to include (used for quick testting). Defaults to 0 which includes all samples. Returns: Dataframe, projects and tags dictionaries. \"\"\" # Load data projects_fp = Path ( config . DATA_DIR , \"projects.json\" ) tags_fp = Path ( config . DATA_DIR , \"tags.json\" ) projects_dict = utils . load_dict ( filepath = projects_fp ) tags_dict = utils . load_dict ( filepath = tags_fp ) # Create dataframe df = pd . DataFrame ( projects_dict ) # Shuffling since projects are chronologically organized if shuffle : df = df . sample ( frac = 1 ) . reset_index ( drop = True ) # Subset if num_samples : df = df [: num_samples ] return df , projects_dict , tags_dict pad_sequences ( sequences , max_seq_len = 0 ) Zero pad sequences to a specified max_seq_len or to the length of the largest sequence in sequences . Usage: # Pad inputs seq = np . array ([[ 1 , 2 , 3 ], [ 1 , 2 ]], dtype = object ) padded_seq = pad_sequences ( sequences = seq , max_seq_len = 5 ) print ( padded_seq ) [[1. 2. 3. 0. 0.] [1. 2. 0. 0. 0.]] Note Input sequences must be 2D. Check out this implemention for a more generalized approach. Parameters: Name Type Description Default sequences ndarray 2D array of data to be padded. required max_seq_len int Length to pad sequences to. Defaults to 0. 0 Exceptions: Type Description ValueError Input sequences are not two-dimensional. Returns: Type Description ndarray An array with the zero padded sequences. Source code in tagifai/data.py def pad_sequences ( sequences : np . ndarray , max_seq_len : int = 0 ) -> np . ndarray : \"\"\"Zero pad sequences to a specified `max_seq_len` or to the length of the largest sequence in `sequences`. Usage: ```python # Pad inputs seq = np.array([[1, 2, 3], [1, 2]], dtype=object) padded_seq = pad_sequences(sequences=seq, max_seq_len=5) print (padded_seq) ``` <pre> [[1. 2. 3. 0. 0.] [1. 2. 0. 0. 0.]] </pre> Note: Input `sequences` must be 2D. Check out this [implemention](https://madewithml.com/courses/ml-foundations/convolutional-neural-networks/#padding){:target=\"_blank\"} for a more generalized approach. Args: sequences (np.ndarray): 2D array of data to be padded. max_seq_len (int, optional): Length to pad sequences to. Defaults to 0. Raises: ValueError: Input sequences are not two-dimensional. Returns: An array with the zero padded sequences. \"\"\" # Check shape if not np . shape ( sequences )[ 0 ] == 2 : raise ValueError ( \"Input sequences are not two-dimensional.\" ) # Get max sequence length max_seq_len = max ( max_seq_len , max ( len ( sequence ) for sequence in sequences ) ) # Pad padded_sequences = np . zeros (( len ( sequences ), max_seq_len )) for i , sequence in enumerate ( sequences ): padded_sequences [ i ][: len ( sequence )] = sequence return padded_sequences preprocess ( text , lower = True , stem = False , filters = '[! \\\\ \" \\' #$%&()* \\\\ +,-./:;<=>?@ \\\\\\\\\\\\ [ \\\\ ]^_`{|}~]' , stopwords = [ 'i' , 'me' , 'my' , 'myself' , 'we' , 'our' , 'ours' , 'ourselves' , 'you' , \"you're\" , \"you've\" , \"you'll\" , \"you'd\" , 'your' , 'yours' , 'yourself' , 'yourselves' , 'he' , 'him' , 'his' , 'himself' , 'she' , \"she's\" , 'her' , 'hers' , 'herself' , 'it' , \"it's\" , 'its' , 'itself' , 'they' , 'them' , 'their' , 'theirs' , 'themselves' , 'what' , 'which' , 'who' , 'whom' , 'this' , 'that' , \"that'll\" , 'these' , 'those' , 'am' , 'is' , 'are' , 'was' , 'were' , 'be' , 'been' , 'being' , 'have' , 'has' , 'had' , 'having' , 'do' , 'does' , 'did' , 'doing' , 'a' , 'an' , 'the' , 'and' , 'but' , 'if' , 'or' , 'because' , 'as' , 'until' , 'while' , 'of' , 'at' , 'by' , 'for' , 'with' , 'about' , 'against' , 'between' , 'into' , 'through' , 'during' , 'before' , 'after' , 'above' , 'below' , 'to' , 'from' , 'up' , 'down' , 'in' , 'out' , 'on' , 'off' , 'over' , 'under' , 'again' , 'further' , 'then' , 'once' , 'here' , 'there' , 'when' , 'where' , 'why' , 'how' , 'all' , 'any' , 'both' , 'each' , 'few' , 'more' , 'most' , 'other' , 'some' , 'such' , 'no' , 'nor' , 'not' , 'only' , 'own' , 'same' , 'so' , 'than' , 'too' , 'very' , 's' , 't' , 'can' , 'will' , 'just' , 'don' , \"don't\" , 'should' , \"should've\" , 'now' , 'd' , 'll' , 'm' , 'o' , 're' , 've' , 'y' , 'ain' , 'aren' , \"aren't\" , 'couldn' , \"couldn't\" , 'didn' , \"didn't\" , 'doesn' , \"doesn't\" , 'hadn' , \"hadn't\" , 'hasn' , \"hasn't\" , 'haven' , \"haven't\" , 'isn' , \"isn't\" , 'ma' , 'mightn' , \"mightn't\" , 'mustn' , \"mustn't\" , 'needn' , \"needn't\" , 'shan' , \"shan't\" , 'shouldn' , \"shouldn't\" , 'wasn' , \"wasn't\" , 'weren' , \"weren't\" , 'won' , \"won't\" , 'wouldn' , \"wouldn't\" ]) Conditional preprocessing on text. Usage: preprocess ( text = \"Transfer learning with BERT!\" , lower = True , stem = True ) 'transfer learn bert' Parameters: Name Type Description Default text str String to preprocess. required lower bool Lower the text. Defaults to True. True stem bool Stem the text. Defaults to False. False filters str Filters to apply on text. '[!\\\\\"\\'#$%&()*\\\\+,-./:;<=>?@\\\\\\\\\\\\[\\\\]^_`{|}~]' stopwords List List of words to filter out. Defaults to STOPWORDS. ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"] Returns: Type Description str Preprocessed string. Source code in tagifai/data.py def preprocess ( text : str , lower : bool = True , stem : bool = False , filters : str = r \"[! \\\" '#$%&()*\\+,-./:;<=>?@ \\\\ \\[\\]^_`{|}~]\" , stopwords : List = STOPWORDS , ) -> str : \"\"\"Conditional preprocessing on text. Usage: ```python preprocess(text=\"Transfer learning with BERT!\", lower=True, stem=True) ``` <pre> 'transfer learn bert' </pre> Args: text (str): String to preprocess. lower (bool, optional): Lower the text. Defaults to True. stem (bool, optional): Stem the text. Defaults to False. filters (str, optional): Filters to apply on text. stopwords (List, optional): List of words to filter out. Defaults to STOPWORDS. Returns: Preprocessed string. \"\"\" # Lower if lower : text = text . lower () # Remove stopwords pattern = re . compile ( r \"\\b(\" + r \"|\" . join ( stopwords ) + r \")\\b\\s*\" ) text = pattern . sub ( \"\" , text ) # Spacing and filters text = re . sub ( r \"([-;;.,!?<=>])\" , r \" \\1 \" , text ) text = re . sub ( filters , r \"\" , text ) text = re . sub ( \"[^A-Za-z0-9]+\" , \" \" , text ) # remove non alphanumeric chars text = re . sub ( \" +\" , \" \" , text ) # remove multiple spaces text = text . strip () # Remove links text = re . sub ( r \"http\\S+\" , \"\" , text ) # Stemming if stem : text = \" \" . join ([ porter . stem ( word ) for word in text . split ( \" \" )]) return text split ( X , y , train_size = 0.7 ) Split data into three (train, val, test) splits. Usage: X_train , X_val , X_test , y_train , y_val , y_test = data . split ( X = X , y = y , train_size = 0.7 ) Parameters: Name Type Description Default X Series input features as a pandas Series object. required y ndarray one-hot encoded labels. required train_size float proportion of data for train split. Defaults to 0.7. 0.7 Returns: Type Description Tuple Inputs and outputs of the three splits (respectively). Source code in tagifai/data.py def split ( X : pd . Series , y : np . ndarray , train_size : float = 0.7 ) -> Tuple : \"\"\"Split data into three (train, val, test) splits. Usage: ```python X_train, X_val, X_test, y_train, y_val, y_test = data.split(X=X, y=y, train_size=0.7) ``` Args: X (pd.Series): input features as a pandas Series object. y (np.ndarray): one-hot encoded labels. train_size (float, optional): proportion of data for train split. Defaults to 0.7. Returns: Inputs and outputs of the three splits (respectively). \"\"\" X_train , X_ , y_train , y_ = iterative_train_test_split ( X , y , train_size = train_size ) X_val , X_test , y_val , y_test = iterative_train_test_split ( X_ , y_ , train_size = 0.5 ) return X_train , X_val , X_test , y_train , y_val , y_test tokenize_text ( X , char_level , tokenizer = None ) Tokenize an array containing text. Usage: # Tokenize inputs X_train , tokenizer = data . tokenize_text ( X = X_train , char_level = True ) X_val , _ = data . tokenize_text ( X = X_val , char_level = True , tokenizer = tokenizer ) X_test , _ = data . tokenize_text ( X = X_test , char_level = True , tokenizer = tokenizer ) Parameters: Name Type Description Default X ndarray Arrays containing the data to tokenize. required char_level bool Whether to tokenize at character level. required tokenizer Tokenizer Tokenizer to use for tokenization. Defaults to None. None Returns: Type Description Tuple Tokenized inputs and tokenizer. Source code in tagifai/data.py def tokenize_text ( X : np . ndarray , char_level : bool , tokenizer : Tokenizer = None ) -> Tuple : \"\"\"Tokenize an array containing text. Usage: ```python # Tokenize inputs X_train, tokenizer = data.tokenize_text(X=X_train, char_level=True) X_val, _ = data.tokenize_text(X=X_val, char_level=True, tokenizer=tokenizer) X_test, _ = data.tokenize_text(X=X_test, char_level=True, tokenizer=tokenizer) ``` Args: X (np.ndarray): Arrays containing the data to tokenize. char_level (bool): Whether to tokenize at character level. tokenizer (Tokenizer): Tokenizer to use for tokenization. Defaults to None. Returns: Tokenized inputs and tokenizer. \"\"\" # Tokenize inputs if not tokenizer : tokenizer = Tokenizer ( char_level = char_level ) tokenizer . fit_on_texts ( texts = X ) X = np . array ( tokenizer . texts_to_sequences ( X )) return X , tokenizer","title":"Data"},{"location":"tagifai/data/#tagifai.data","text":"","title":"tagifai.data"},{"location":"tagifai/data/#classes","text":"","title":"Classes"},{"location":"tagifai/data/#tagifai.data.CNNTextDataset","text":"Create torch.utils.data.Dataset objects to use for efficiently feeding data into our models. Usage: # Create dataset X , y = data dataset = CNNTextDataset ( X = X , y = y , max_filter_size = max_filter_size ) # Create dataloaders dataloader = dataset . create_dataloader ( batch_size = batch_size )","title":"CNNTextDataset"},{"location":"tagifai/data/#methods","text":"","title":"Methods"},{"location":"tagifai/data/#tagifai.data.LabelEncoder","text":"Encode labels into unique indices. Usage: # Encode labels label_encoder = LabelEncoder () label_encoder . fit ( labels ) y = label_encoder . encode ( labels )","title":"LabelEncoder"},{"location":"tagifai/data/#methods_1","text":"","title":"Methods"},{"location":"tagifai/data/#tagifai.data.Tokenizer","text":"Tokenize a feature using a built vocabulary. Usage: tokenizer = Tokenizer ( char_level = char_level ) tokenizer . fit_on_texts ( texts = X ) X = np . array ( tokenizer . texts_to_sequences ( X ))","title":"Tokenizer"},{"location":"tagifai/data/#methods_2","text":"","title":"Methods"},{"location":"tagifai/data/#functions","text":"","title":"Functions"},{"location":"tagifai/data/#tagifai.data.clean","text":"Cleaning the raw data. Parameters: Name Type Description Default df DataFrame Pandas DataFrame with data. required tags_dict Dict Dictionary of tags and their metadata (parents, aliases, etc.) required min_tag_freq int Minimum frequency of tags required. Defaults to 30. 30 Returns: Type Description Tuple A cleaned dataframe, dictionary of all tags and tags above the frequency threshold. Source code in tagifai/data.py def clean ( df : pd . DataFrame , tags_dict : Dict , min_tag_freq : int = 30 ) -> Tuple : \"\"\"Cleaning the raw data. Args: df (pd.DataFrame): Pandas DataFrame with data. tags_dict (Dict): Dictionary of tags and their metadata (parents, aliases, etc.) min_tag_freq (int, optional): Minimum frequency of tags required. Defaults to 30. Returns: A cleaned dataframe, dictionary of all tags and tags above the frequency threshold. \"\"\" # Combine features df [ \"text\" ] = df . title + \" \" + df . description # Inclusion/exclusion criteria for tags include = list ( tags_dict . keys ()) exclude = [ \"machine-learning\" , \"deep-learning\" , \"data-science\" , \"neural-networks\" , \"python\" , \"r\" , \"visualization\" , ] # Filter tags for each project df . tags = df . tags . apply ( filter_items , include = include , exclude = exclude ) tags = Counter ( itertools . chain . from_iterable ( df . tags . values )) # Filter tags that have fewer than `min_tag_freq` occurrences tags_above_freq = Counter ( tag for tag in tags . elements () if tags [ tag ] >= min_tag_freq ) df . tags = df . tags . apply ( filter_items , include = list ( tags_above_freq . keys ())) # Remove projects with no more remaining relevant tags df = df [ df . tags . map ( len ) > 0 ] return df , tags_dict , tags_above_freq","title":"clean()"},{"location":"tagifai/data/#tagifai.data.encode_labels","text":"Encode labels into unique integers. Usage: y , class_weights , label_encoder = data . encode_labels ( labels = df . tags ) Parameters: Name Type Description Default labels Series Pandas Series of all the labels. required Returns: Type Description tuple Encoded labels, class weights and the encoder. Source code in tagifai/data.py def encode_labels ( labels : pd . Series ) -> tuple : \"\"\"Encode labels into unique integers. Usage: ```python y, class_weights, label_encoder = data.encode_labels(labels=df.tags) ``` Args: labels (pd.Series): Pandas Series of all the labels. Returns: Encoded labels, class weights and the encoder. \"\"\" # Encode labels label_encoder = LabelEncoder () label_encoder . fit ( labels ) y = label_encoder . encode ( labels ) # Class weights all_tags = list ( itertools . chain . from_iterable ( labels . values )) counts = np . bincount ( [ label_encoder . class_to_index [ class_ ] for class_ in all_tags ] ) class_weights = { i : 1.0 / count for i , count in enumerate ( counts )} return y , class_weights , label_encoder","title":"encode_labels()"},{"location":"tagifai/data/#tagifai.data.filter_items","text":"Filter a list using inclusion and exclusion lists of items. Parameters: Name Type Description Default items List List of items to apply filters. required include List List of items to include. Defaults to []. [] exclude List List of items to filter out. Defaults to []. [] Returns: Type Description List Filtered list of items. # Inclusion/exclusion criteria for tags include = list ( tags_dict . keys ()) exclude = [ \"machine-learning\" , \"deep-learning\" , \"data-science\" , \"neural-networks\" , \"python\" , \"r\" , \"visualization\" , ] # Filter tags for each project df . tags = df . tags . apply ( filter_items , include = include , exclude = exclude ) Source code in tagifai/data.py def filter_items ( items : List , include : List = [], exclude : List = []) -> List : \"\"\"Filter a list using inclusion and exclusion lists of items. Args: items (List): List of items to apply filters. include (List, optional): List of items to include. Defaults to []. exclude (List, optional): List of items to filter out. Defaults to []. Returns: Filtered list of items. Usage: ```python # Inclusion/exclusion criteria for tags include = list(tags_dict.keys()) exclude = [ \"machine-learning\", \"deep-learning\", \"data-science\", \"neural-networks\", \"python\", \"r\", \"visualization\", ] # Filter tags for each project df.tags = df.tags.apply(filter_items, include=include, exclude=exclude) ``` \"\"\" filtered = [ item for item in items if item in include and item not in exclude ] return filtered","title":"filter_items()"},{"location":"tagifai/data/#tagifai.data.get_dataloader","text":"Create dataloader from data. Parameters: Name Type Description Default data Tuple Data to load into the DataLoader object. required max_filter_size int length of the largest CNN filter. required batch_size int number of samples per batch. required Returns: Type Description Tuple Created dataloader with data. Source code in tagifai/data.py def get_dataloader ( data : Tuple , max_filter_size : int , batch_size : int ) -> Tuple : \"\"\"Create dataloader from data. Args: data (Tuple): Data to load into the DataLoader object. max_filter_size (int): length of the largest CNN filter. batch_size (int): number of samples per batch. Returns: Created dataloader with data. \"\"\" # Create dataset X , y = data dataset = CNNTextDataset ( X = X , y = y , max_filter_size = max_filter_size ) # Create dataloaders dataloader = dataset . create_dataloader ( batch_size = batch_size ) return dataloader","title":"get_dataloader()"},{"location":"tagifai/data/#tagifai.data.iterative_train_test_split","text":"Custom iterative train test split which 'maintains balanced representation with respect to order-th label combinations.' Parameters: Name Type Description Default X Series Input features as a pandas Series object. required y ndarray One-hot encoded labels. required train_size float Proportion of data for first split. Defaults to 0.7. 0.7 Returns: Type Description Tuple Two stratified splits based on specified proportions. Source code in tagifai/data.py def iterative_train_test_split ( X : pd . Series , y : np . ndarray , train_size : float = 0.7 ) -> Tuple : \"\"\"Custom iterative train test split which 'maintains balanced representation with respect to order-th label combinations.' Args: X (pd.Series): Input features as a pandas Series object. y (np.ndarray): One-hot encoded labels. train_size (float, optional): Proportion of data for first split. Defaults to 0.7. Returns: Two stratified splits based on specified proportions. \"\"\" stratifier = IterativeStratification ( n_splits = 2 , order = 1 , sample_distribution_per_fold = [ 1.0 - train_size , train_size , ], ) train_indices , test_indices = next ( stratifier . split ( X , y )) X_train , y_train = X [ train_indices ], y [ train_indices ] X_test , y_test = X [ test_indices ], y [ test_indices ] return X_train , X_test , y_train , y_test","title":"iterative_train_test_split()"},{"location":"tagifai/data/#tagifai.data.load","text":"Load the data from local drive to a Pandas DataFrame. Parameters: Name Type Description Default shuffle bool Shuffle the data. required num_samples int Number of samples to include (used for quick testting). Defaults to 0 which includes all samples. 0 Returns: Type Description DataFrame Dataframe, projects and tags dictionaries. Source code in tagifai/data.py def load ( shuffle : bool , num_samples : int = 0 ) -> pd . DataFrame : \"\"\"Load the data from local drive to a Pandas DataFrame. Args: shuffle (bool): Shuffle the data. num_samples (int, optional): Number of samples to include (used for quick testting). Defaults to 0 which includes all samples. Returns: Dataframe, projects and tags dictionaries. \"\"\" # Load data projects_fp = Path ( config . DATA_DIR , \"projects.json\" ) tags_fp = Path ( config . DATA_DIR , \"tags.json\" ) projects_dict = utils . load_dict ( filepath = projects_fp ) tags_dict = utils . load_dict ( filepath = tags_fp ) # Create dataframe df = pd . DataFrame ( projects_dict ) # Shuffling since projects are chronologically organized if shuffle : df = df . sample ( frac = 1 ) . reset_index ( drop = True ) # Subset if num_samples : df = df [: num_samples ] return df , projects_dict , tags_dict","title":"load()"},{"location":"tagifai/data/#tagifai.data.pad_sequences","text":"Zero pad sequences to a specified max_seq_len or to the length of the largest sequence in sequences . Usage: # Pad inputs seq = np . array ([[ 1 , 2 , 3 ], [ 1 , 2 ]], dtype = object ) padded_seq = pad_sequences ( sequences = seq , max_seq_len = 5 ) print ( padded_seq ) [[1. 2. 3. 0. 0.] [1. 2. 0. 0. 0.]] Note Input sequences must be 2D. Check out this implemention for a more generalized approach. Parameters: Name Type Description Default sequences ndarray 2D array of data to be padded. required max_seq_len int Length to pad sequences to. Defaults to 0. 0 Exceptions: Type Description ValueError Input sequences are not two-dimensional. Returns: Type Description ndarray An array with the zero padded sequences. Source code in tagifai/data.py def pad_sequences ( sequences : np . ndarray , max_seq_len : int = 0 ) -> np . ndarray : \"\"\"Zero pad sequences to a specified `max_seq_len` or to the length of the largest sequence in `sequences`. Usage: ```python # Pad inputs seq = np.array([[1, 2, 3], [1, 2]], dtype=object) padded_seq = pad_sequences(sequences=seq, max_seq_len=5) print (padded_seq) ``` <pre> [[1. 2. 3. 0. 0.] [1. 2. 0. 0. 0.]] </pre> Note: Input `sequences` must be 2D. Check out this [implemention](https://madewithml.com/courses/ml-foundations/convolutional-neural-networks/#padding){:target=\"_blank\"} for a more generalized approach. Args: sequences (np.ndarray): 2D array of data to be padded. max_seq_len (int, optional): Length to pad sequences to. Defaults to 0. Raises: ValueError: Input sequences are not two-dimensional. Returns: An array with the zero padded sequences. \"\"\" # Check shape if not np . shape ( sequences )[ 0 ] == 2 : raise ValueError ( \"Input sequences are not two-dimensional.\" ) # Get max sequence length max_seq_len = max ( max_seq_len , max ( len ( sequence ) for sequence in sequences ) ) # Pad padded_sequences = np . zeros (( len ( sequences ), max_seq_len )) for i , sequence in enumerate ( sequences ): padded_sequences [ i ][: len ( sequence )] = sequence return padded_sequences","title":"pad_sequences()"},{"location":"tagifai/data/#tagifai.data.preprocess","text":"Conditional preprocessing on text. Usage: preprocess ( text = \"Transfer learning with BERT!\" , lower = True , stem = True ) 'transfer learn bert' Parameters: Name Type Description Default text str String to preprocess. required lower bool Lower the text. Defaults to True. True stem bool Stem the text. Defaults to False. False filters str Filters to apply on text. '[!\\\\\"\\'#$%&()*\\\\+,-./:;<=>?@\\\\\\\\\\\\[\\\\]^_`{|}~]' stopwords List List of words to filter out. Defaults to STOPWORDS. ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"] Returns: Type Description str Preprocessed string. Source code in tagifai/data.py def preprocess ( text : str , lower : bool = True , stem : bool = False , filters : str = r \"[! \\\" '#$%&()*\\+,-./:;<=>?@ \\\\ \\[\\]^_`{|}~]\" , stopwords : List = STOPWORDS , ) -> str : \"\"\"Conditional preprocessing on text. Usage: ```python preprocess(text=\"Transfer learning with BERT!\", lower=True, stem=True) ``` <pre> 'transfer learn bert' </pre> Args: text (str): String to preprocess. lower (bool, optional): Lower the text. Defaults to True. stem (bool, optional): Stem the text. Defaults to False. filters (str, optional): Filters to apply on text. stopwords (List, optional): List of words to filter out. Defaults to STOPWORDS. Returns: Preprocessed string. \"\"\" # Lower if lower : text = text . lower () # Remove stopwords pattern = re . compile ( r \"\\b(\" + r \"|\" . join ( stopwords ) + r \")\\b\\s*\" ) text = pattern . sub ( \"\" , text ) # Spacing and filters text = re . sub ( r \"([-;;.,!?<=>])\" , r \" \\1 \" , text ) text = re . sub ( filters , r \"\" , text ) text = re . sub ( \"[^A-Za-z0-9]+\" , \" \" , text ) # remove non alphanumeric chars text = re . sub ( \" +\" , \" \" , text ) # remove multiple spaces text = text . strip () # Remove links text = re . sub ( r \"http\\S+\" , \"\" , text ) # Stemming if stem : text = \" \" . join ([ porter . stem ( word ) for word in text . split ( \" \" )]) return text","title":"preprocess()"},{"location":"tagifai/data/#tagifai.data.split","text":"Split data into three (train, val, test) splits. Usage: X_train , X_val , X_test , y_train , y_val , y_test = data . split ( X = X , y = y , train_size = 0.7 ) Parameters: Name Type Description Default X Series input features as a pandas Series object. required y ndarray one-hot encoded labels. required train_size float proportion of data for train split. Defaults to 0.7. 0.7 Returns: Type Description Tuple Inputs and outputs of the three splits (respectively). Source code in tagifai/data.py def split ( X : pd . Series , y : np . ndarray , train_size : float = 0.7 ) -> Tuple : \"\"\"Split data into three (train, val, test) splits. Usage: ```python X_train, X_val, X_test, y_train, y_val, y_test = data.split(X=X, y=y, train_size=0.7) ``` Args: X (pd.Series): input features as a pandas Series object. y (np.ndarray): one-hot encoded labels. train_size (float, optional): proportion of data for train split. Defaults to 0.7. Returns: Inputs and outputs of the three splits (respectively). \"\"\" X_train , X_ , y_train , y_ = iterative_train_test_split ( X , y , train_size = train_size ) X_val , X_test , y_val , y_test = iterative_train_test_split ( X_ , y_ , train_size = 0.5 ) return X_train , X_val , X_test , y_train , y_val , y_test","title":"split()"},{"location":"tagifai/data/#tagifai.data.tokenize_text","text":"Tokenize an array containing text. Usage: # Tokenize inputs X_train , tokenizer = data . tokenize_text ( X = X_train , char_level = True ) X_val , _ = data . tokenize_text ( X = X_val , char_level = True , tokenizer = tokenizer ) X_test , _ = data . tokenize_text ( X = X_test , char_level = True , tokenizer = tokenizer ) Parameters: Name Type Description Default X ndarray Arrays containing the data to tokenize. required char_level bool Whether to tokenize at character level. required tokenizer Tokenizer Tokenizer to use for tokenization. Defaults to None. None Returns: Type Description Tuple Tokenized inputs and tokenizer. Source code in tagifai/data.py def tokenize_text ( X : np . ndarray , char_level : bool , tokenizer : Tokenizer = None ) -> Tuple : \"\"\"Tokenize an array containing text. Usage: ```python # Tokenize inputs X_train, tokenizer = data.tokenize_text(X=X_train, char_level=True) X_val, _ = data.tokenize_text(X=X_val, char_level=True, tokenizer=tokenizer) X_test, _ = data.tokenize_text(X=X_test, char_level=True, tokenizer=tokenizer) ``` Args: X (np.ndarray): Arrays containing the data to tokenize. char_level (bool): Whether to tokenize at character level. tokenizer (Tokenizer): Tokenizer to use for tokenization. Defaults to None. Returns: Tokenized inputs and tokenizer. \"\"\" # Tokenize inputs if not tokenizer : tokenizer = Tokenizer ( char_level = char_level ) tokenizer . fit_on_texts ( texts = X ) X = np . array ( tokenizer . texts_to_sequences ( X )) return X , tokenizer","title":"tokenize_text()"},{"location":"tagifai/main/","text":"All the functions here can be used as a CLI command thanks to your Typer application. # View all Typer commands $ tagifai --help Usage: tagifai [ OPTIONS ] COMMAND [ ARGS ] \ud83d\udc49 Commands: download-data Download data from online to local drive. optimize Optimize a subset of hyperparameters towards ... train-model Predict tags for a give input text using a ... predict-tags Train a model using the specified parameters. View individual commands and their arguments: # Help for a specific command $ tagifai train-model --help Usage: tagifai train-model [ OPTIONS ] Options: --args-fp PATH [ default: config/args.json ] --help Show this message and exit. # Train a model $ tagifai train-model --args-fp $PATH \ud83d\ude80 Training... Functions download_data () Download data from online to local drive. Note We could've just copied files from datasets but we'll use this later on with other data sources. Source code in tagifai/main.py @app . command () def download_data (): \"\"\"Download data from online to local drive. Note: We could've just copied files from `datasets` but we'll use this later on with other data sources. \"\"\" # Download data projects_url = \"https://raw.githubusercontent.com/GokuMohandas/applied-ml/main/datasets/projects.json\" tags_url = \"https://raw.githubusercontent.com/GokuMohandas/applied-ml/main/datasets/tags.json\" projects = utils . load_json_from_url ( url = projects_url ) tags = utils . load_json_from_url ( url = tags_url ) # Save data projects_fp = Path ( config . DATA_DIR , \"projects.json\" ) tags_fp = Path ( config . DATA_DIR , \"tags.json\" ) utils . save_dict ( d = projects , filepath = projects_fp ) utils . save_dict ( d = tags , filepath = tags_fp ) logger . info ( \"\u2705 Data downloaded!\" ) optimize ( num_trials = 100 ) Optimize a subset of hyperparameters towards an objective. This saves the best trial's arguments into config/args.json . Parameters: Name Type Description Default num_trials int Number of trials to run. Defaults to 100. 100 Source code in tagifai/main.py @app . command () def optimize ( num_trials : int = 100 ) -> None : \"\"\"Optimize a subset of hyperparameters towards an objective. This saves the best trial's arguments into `config/args.json`. Args: num_trials (int, optional): Number of trials to run. Defaults to 100. \"\"\" # Starting arguments (not actually used but needed for set up) args_fp = Path ( config . CONFIG_DIR , \"args.json\" ) args = Namespace ( ** utils . load_dict ( filepath = args_fp )) # Optimize pruner = optuna . pruners . MedianPruner ( n_startup_trials = 5 , n_warmup_steps = 5 ) study = optuna . create_study ( study_name = \"optimization\" , direction = \"maximize\" , pruner = pruner ) mlflow_callback = MLflowCallback ( tracking_uri = mlflow . get_tracking_uri (), metric_name = \"f1\" ) study . optimize ( lambda trial : train . objective ( args , trial ), n_trials = num_trials , callbacks = [ mlflow_callback ], ) # All trials trials_df = study . trials_dataframe () trials_df = trials_df . sort_values ( [ \"value\" ], ascending = False ) # sort by metric trials_df . to_csv ( Path ( config . EXPERIMENTS_DIR , \"trials.csv\" ), index = False ) # save # Best trial logger . info ( f \"Best value (f1): { study . best_trial . value } \" ) params = { ** args . __dict__ , ** study . best_trial . params } params [ \"threshold\" ] = study . best_trial . user_attrs [ \"threshold\" ] with open ( Path ( config . CONFIG_DIR , \"args.json\" ), \"w\" ) as fp : json . dump ( params , fp = fp , indent = 2 , cls = NumpyEncoder ) logger . info ( json . dumps ( params , indent = 2 , cls = NumpyEncoder )) predict_tags ( text = 'Transfer learning with BERT for self-supervised learning' , run_id = '' ) Predict tags for a give input text using a trained model. Warning Make sure that you have a trained model first! Parameters: Name Type Description Default text str Input text to predict tags for. Defaults to \"Transfer learning with BERT for self-supervised learning\". 'Transfer learning with BERT for self-supervised learning' run_id str ID of the run to load model artifacts from. Defaults to model with lowest best_val_loss from the best experiment. '' Returns: Type Description Dict Predicted tags for input text. Source code in tagifai/main.py @app . command () def predict_tags ( text : str = \"Transfer learning with BERT for self-supervised learning\" , run_id : str = \"\" , ) -> Dict : \"\"\"Predict tags for a give input text using a trained model. Warning: Make sure that you have a trained model first! Args: text (str, optional): Input text to predict tags for. Defaults to \"Transfer learning with BERT for self-supervised learning\". run_id (str, optional): ID of the run to load model artifacts from. Defaults to model with lowest `best_val_loss` from the `best` experiment. Returns: Predicted tags for input text. \"\"\" # Get best run if not run_id : experiment_id = mlflow . get_experiment_by_name ( \"best\" ) . experiment_id all_runs = mlflow . search_runs ( experiment_ids = experiment_id , order_by = [ \"metrics.best_val_loss ASC\" ], ) run_id = all_runs . iloc [ 0 ] . run_id # Predict prediction = predict . predict ( texts = [ text ], run_id = run_id ) logger . info ( json . dumps ( prediction , indent = 2 )) return prediction set_artifact_metadata () Set the artifact URI for all experiments and runs. Used when transferring experiments from other locations (ex. Colab). Note check out the optimize.ipynb notebook for how to train on Google Colab and transfer to local. Source code in tagifai/main.py @app . command () def set_artifact_metadata (): \"\"\"Set the artifact URI for all experiments and runs. Used when transferring experiments from other locations (ex. Colab). Note: check out the [optimize.ipynb](https://colab.research.google.com/github/GokuMohandas/applied-ml/blob/main/notebooks/optimize.ipynb){:target=\"_blank\"} notebook for how to train on Google Colab and transfer to local. \"\"\" def set_artifact_location ( var , fp ): \"\"\"Set variable's yaml value on file at fp.\"\"\" with open ( fp ) as f : metadata = yaml . load ( f ) # Set new value experiment_id = metadata [ var ] . split ( \"/\" )[ - 1 ] artifact_location = Path ( \"file://\" , config . EXPERIMENTS_DIR , experiment_id ) metadata [ var ] = str ( artifact_location ) with open ( fp , \"w\" ) as f : yaml . dump ( metadata , f ) def set_artifact_uri ( var , fp ): \"\"\"Set variable's yaml value on file at fp.\"\"\" with open ( fp ) as f : metadata = yaml . load ( f ) # Set new value experiment_id = metadata [ var ] . split ( \"/\" )[ - 3 ] run_id = metadata [ var ] . split ( \"/\" )[ - 2 ] artifact_uri = Path ( \"file://\" , config . EXPERIMENTS_DIR , experiment_id , run_id , \"artifacts\" , ) metadata [ var ] = str ( artifact_uri ) with open ( fp , \"w\" ) as f : yaml . dump ( metadata , f ) # Get artifact location experiment_meta_yamls = list ( Path ( config . EXPERIMENTS_DIR ) . glob ( \"*/meta.yaml\" ) ) for meta_yaml in experiment_meta_yamls : set_artifact_location ( var = \"artifact_location\" , fp = meta_yaml ) # Change artifact URI run_meta_yamls = list ( Path ( config . EXPERIMENTS_DIR ) . glob ( \"*/*/meta.yaml\" )) for meta_yaml in run_meta_yamls : set_artifact_uri ( var = \"artifact_uri\" , fp = meta_yaml ) train_model ( args_fp = PosixPath ( '/home/runner/work/applied-ml/applied-ml/config/args.json' )) Train a model using the specified parameters. Parameters: Name Type Description Default args_fp Path Location of arguments to use for training. Defaults to config/args.json . PosixPath('/home/runner/work/applied-ml/applied-ml/config/args.json') Source code in tagifai/main.py @app . command () def train_model ( args_fp : Path = Path ( config . CONFIG_DIR , \"args.json\" )) -> None : \"\"\"Train a model using the specified parameters. Args: args_fp (Path, optional): Location of arguments to use for training. Defaults to `config/args.json`. \"\"\" # Set experiment and start run args = Namespace ( ** utils . load_dict ( filepath = args_fp )) # Start run mlflow . set_experiment ( experiment_name = \"best\" ) with mlflow . start_run ( run_name = \"cnn\" ) as run : # NOQA: F841 (assigned to but never used) # Train artifacts = train . run ( args = args ) # Log key metrics performance = artifacts [ \"performance\" ] loss = artifacts [ \"loss\" ] mlflow . log_metrics ({ \"precision\" : performance [ \"overall\" ][ \"precision\" ]}) mlflow . log_metrics ({ \"recall\" : performance [ \"overall\" ][ \"recall\" ]}) mlflow . log_metrics ({ \"f1\" : performance [ \"overall\" ][ \"f1\" ]}) mlflow . log_metrics ({ \"best_val_loss\" : loss }) # Log artifacts args = artifacts [ \"args\" ] model = artifacts [ \"model\" ] label_encoder = artifacts [ \"label_encoder\" ] tokenizer = artifacts [ \"tokenizer\" ] with tempfile . TemporaryDirectory () as fp : label_encoder . save ( Path ( fp , \"label_encoder.json\" )) tokenizer . save ( Path ( fp , \"tokenizer.json\" )) torch . save ( model . state_dict (), Path ( fp , \"model.pt\" )) utils . save_dict ( performance , Path ( fp , \"performance.json\" )) mlflow . log_artifacts ( fp ) mlflow . log_params ( vars ( args )) logger . info ( json . dumps ( performance [ \"overall\" ], indent = 2 ))","title":"CLI"},{"location":"tagifai/main/#tagifai.main","text":"","title":"tagifai.main"},{"location":"tagifai/main/#functions","text":"","title":"Functions"},{"location":"tagifai/main/#tagifai.main.download_data","text":"Download data from online to local drive. Note We could've just copied files from datasets but we'll use this later on with other data sources. Source code in tagifai/main.py @app . command () def download_data (): \"\"\"Download data from online to local drive. Note: We could've just copied files from `datasets` but we'll use this later on with other data sources. \"\"\" # Download data projects_url = \"https://raw.githubusercontent.com/GokuMohandas/applied-ml/main/datasets/projects.json\" tags_url = \"https://raw.githubusercontent.com/GokuMohandas/applied-ml/main/datasets/tags.json\" projects = utils . load_json_from_url ( url = projects_url ) tags = utils . load_json_from_url ( url = tags_url ) # Save data projects_fp = Path ( config . DATA_DIR , \"projects.json\" ) tags_fp = Path ( config . DATA_DIR , \"tags.json\" ) utils . save_dict ( d = projects , filepath = projects_fp ) utils . save_dict ( d = tags , filepath = tags_fp ) logger . info ( \"\u2705 Data downloaded!\" )","title":"download_data()"},{"location":"tagifai/main/#tagifai.main.optimize","text":"Optimize a subset of hyperparameters towards an objective. This saves the best trial's arguments into config/args.json . Parameters: Name Type Description Default num_trials int Number of trials to run. Defaults to 100. 100 Source code in tagifai/main.py @app . command () def optimize ( num_trials : int = 100 ) -> None : \"\"\"Optimize a subset of hyperparameters towards an objective. This saves the best trial's arguments into `config/args.json`. Args: num_trials (int, optional): Number of trials to run. Defaults to 100. \"\"\" # Starting arguments (not actually used but needed for set up) args_fp = Path ( config . CONFIG_DIR , \"args.json\" ) args = Namespace ( ** utils . load_dict ( filepath = args_fp )) # Optimize pruner = optuna . pruners . MedianPruner ( n_startup_trials = 5 , n_warmup_steps = 5 ) study = optuna . create_study ( study_name = \"optimization\" , direction = \"maximize\" , pruner = pruner ) mlflow_callback = MLflowCallback ( tracking_uri = mlflow . get_tracking_uri (), metric_name = \"f1\" ) study . optimize ( lambda trial : train . objective ( args , trial ), n_trials = num_trials , callbacks = [ mlflow_callback ], ) # All trials trials_df = study . trials_dataframe () trials_df = trials_df . sort_values ( [ \"value\" ], ascending = False ) # sort by metric trials_df . to_csv ( Path ( config . EXPERIMENTS_DIR , \"trials.csv\" ), index = False ) # save # Best trial logger . info ( f \"Best value (f1): { study . best_trial . value } \" ) params = { ** args . __dict__ , ** study . best_trial . params } params [ \"threshold\" ] = study . best_trial . user_attrs [ \"threshold\" ] with open ( Path ( config . CONFIG_DIR , \"args.json\" ), \"w\" ) as fp : json . dump ( params , fp = fp , indent = 2 , cls = NumpyEncoder ) logger . info ( json . dumps ( params , indent = 2 , cls = NumpyEncoder ))","title":"optimize()"},{"location":"tagifai/main/#tagifai.main.predict_tags","text":"Predict tags for a give input text using a trained model. Warning Make sure that you have a trained model first! Parameters: Name Type Description Default text str Input text to predict tags for. Defaults to \"Transfer learning with BERT for self-supervised learning\". 'Transfer learning with BERT for self-supervised learning' run_id str ID of the run to load model artifacts from. Defaults to model with lowest best_val_loss from the best experiment. '' Returns: Type Description Dict Predicted tags for input text. Source code in tagifai/main.py @app . command () def predict_tags ( text : str = \"Transfer learning with BERT for self-supervised learning\" , run_id : str = \"\" , ) -> Dict : \"\"\"Predict tags for a give input text using a trained model. Warning: Make sure that you have a trained model first! Args: text (str, optional): Input text to predict tags for. Defaults to \"Transfer learning with BERT for self-supervised learning\". run_id (str, optional): ID of the run to load model artifacts from. Defaults to model with lowest `best_val_loss` from the `best` experiment. Returns: Predicted tags for input text. \"\"\" # Get best run if not run_id : experiment_id = mlflow . get_experiment_by_name ( \"best\" ) . experiment_id all_runs = mlflow . search_runs ( experiment_ids = experiment_id , order_by = [ \"metrics.best_val_loss ASC\" ], ) run_id = all_runs . iloc [ 0 ] . run_id # Predict prediction = predict . predict ( texts = [ text ], run_id = run_id ) logger . info ( json . dumps ( prediction , indent = 2 )) return prediction","title":"predict_tags()"},{"location":"tagifai/main/#tagifai.main.set_artifact_metadata","text":"Set the artifact URI for all experiments and runs. Used when transferring experiments from other locations (ex. Colab). Note check out the optimize.ipynb notebook for how to train on Google Colab and transfer to local. Source code in tagifai/main.py @app . command () def set_artifact_metadata (): \"\"\"Set the artifact URI for all experiments and runs. Used when transferring experiments from other locations (ex. Colab). Note: check out the [optimize.ipynb](https://colab.research.google.com/github/GokuMohandas/applied-ml/blob/main/notebooks/optimize.ipynb){:target=\"_blank\"} notebook for how to train on Google Colab and transfer to local. \"\"\" def set_artifact_location ( var , fp ): \"\"\"Set variable's yaml value on file at fp.\"\"\" with open ( fp ) as f : metadata = yaml . load ( f ) # Set new value experiment_id = metadata [ var ] . split ( \"/\" )[ - 1 ] artifact_location = Path ( \"file://\" , config . EXPERIMENTS_DIR , experiment_id ) metadata [ var ] = str ( artifact_location ) with open ( fp , \"w\" ) as f : yaml . dump ( metadata , f ) def set_artifact_uri ( var , fp ): \"\"\"Set variable's yaml value on file at fp.\"\"\" with open ( fp ) as f : metadata = yaml . load ( f ) # Set new value experiment_id = metadata [ var ] . split ( \"/\" )[ - 3 ] run_id = metadata [ var ] . split ( \"/\" )[ - 2 ] artifact_uri = Path ( \"file://\" , config . EXPERIMENTS_DIR , experiment_id , run_id , \"artifacts\" , ) metadata [ var ] = str ( artifact_uri ) with open ( fp , \"w\" ) as f : yaml . dump ( metadata , f ) # Get artifact location experiment_meta_yamls = list ( Path ( config . EXPERIMENTS_DIR ) . glob ( \"*/meta.yaml\" ) ) for meta_yaml in experiment_meta_yamls : set_artifact_location ( var = \"artifact_location\" , fp = meta_yaml ) # Change artifact URI run_meta_yamls = list ( Path ( config . EXPERIMENTS_DIR ) . glob ( \"*/*/meta.yaml\" )) for meta_yaml in run_meta_yamls : set_artifact_uri ( var = \"artifact_uri\" , fp = meta_yaml )","title":"set_artifact_metadata()"},{"location":"tagifai/main/#tagifai.main.train_model","text":"Train a model using the specified parameters. Parameters: Name Type Description Default args_fp Path Location of arguments to use for training. Defaults to config/args.json . PosixPath('/home/runner/work/applied-ml/applied-ml/config/args.json') Source code in tagifai/main.py @app . command () def train_model ( args_fp : Path = Path ( config . CONFIG_DIR , \"args.json\" )) -> None : \"\"\"Train a model using the specified parameters. Args: args_fp (Path, optional): Location of arguments to use for training. Defaults to `config/args.json`. \"\"\" # Set experiment and start run args = Namespace ( ** utils . load_dict ( filepath = args_fp )) # Start run mlflow . set_experiment ( experiment_name = \"best\" ) with mlflow . start_run ( run_name = \"cnn\" ) as run : # NOQA: F841 (assigned to but never used) # Train artifacts = train . run ( args = args ) # Log key metrics performance = artifacts [ \"performance\" ] loss = artifacts [ \"loss\" ] mlflow . log_metrics ({ \"precision\" : performance [ \"overall\" ][ \"precision\" ]}) mlflow . log_metrics ({ \"recall\" : performance [ \"overall\" ][ \"recall\" ]}) mlflow . log_metrics ({ \"f1\" : performance [ \"overall\" ][ \"f1\" ]}) mlflow . log_metrics ({ \"best_val_loss\" : loss }) # Log artifacts args = artifacts [ \"args\" ] model = artifacts [ \"model\" ] label_encoder = artifacts [ \"label_encoder\" ] tokenizer = artifacts [ \"tokenizer\" ] with tempfile . TemporaryDirectory () as fp : label_encoder . save ( Path ( fp , \"label_encoder.json\" )) tokenizer . save ( Path ( fp , \"tokenizer.json\" )) torch . save ( model . state_dict (), Path ( fp , \"model.pt\" )) utils . save_dict ( performance , Path ( fp , \"performance.json\" )) mlflow . log_artifacts ( fp ) mlflow . log_params ( vars ( args )) logger . info ( json . dumps ( performance [ \"overall\" ], indent = 2 ))","title":"train_model()"},{"location":"tagifai/models/","text":"Classes CNN Methods __init__ ( self , embedding_dim , vocab_size , num_filters , filter_sizes , hidden_dim , dropout_p , num_classes , padding_idx = 0 ) special A convolutional neural network architecture created for natural language processing tasks where filters convolve across the given text inputs. Usage: # Initialize model filter_sizes = list ( range ( 1 , int ( args . max_filter_size ) + 1 )) model = models . CNN ( embedding_dim = int ( args . embedding_dim ), vocab_size = int ( vocab_size ), num_filters = int ( args . num_filters ), filter_sizes = filter_sizes , hidden_dim = int ( args . hidden_dim ), dropout_p = float ( args . dropout_p ), num_classes = int ( num_classes ), ) model = model . to ( device ) Parameters: Name Type Description Default embedding_dim int Embedding dimension for tokens. required vocab_size int Number of unique tokens in vocabulary. required num_filters int Number of filters per filter size. required filter_sizes list List of filter sizes for the CNN. required hidden_dim int Hidden dimension for fully-connected (FC) layers. required dropout_p float Dropout proportion for FC layers. required num_classes int Number of unique classes to classify into. required padding_idx int Index representing the <PAD> token. Defaults to 0. 0 Source code in tagifai/models.py def __init__ ( self , embedding_dim : int , vocab_size : int , num_filters : int , filter_sizes : list , hidden_dim : int , dropout_p : float , num_classes : int , padding_idx : int = 0 , ) -> None : \"\"\"A [convolutional neural network](https://madewithml.com/courses/ml-foundations/convolutional-neural-networks/){:target=\"_blank\"} architecture created for natural language processing tasks where filters convolve across the given text inputs. ![text CNN](https://raw.githubusercontent.com/GokuMohandas/madewithml/main/images/ml-foundations/embeddings/model.png) Usage: ```python # Initialize model filter_sizes = list(range(1, int(args.max_filter_size) + 1)) model = models.CNN( embedding_dim=int(args.embedding_dim), vocab_size=int(vocab_size), num_filters=int(args.num_filters), filter_sizes=filter_sizes, hidden_dim=int(args.hidden_dim), dropout_p=float(args.dropout_p), num_classes=int(num_classes), ) model = model.to(device) ``` Args: embedding_dim (int): Embedding dimension for tokens. vocab_size (int): Number of unique tokens in vocabulary. num_filters (int): Number of filters per filter size. filter_sizes (list): List of filter sizes for the CNN. hidden_dim (int): Hidden dimension for fully-connected (FC) layers. dropout_p (float): Dropout proportion for FC layers. num_classes (int): Number of unique classes to classify into. padding_idx (int, optional): Index representing the `<PAD>` token. Defaults to 0. \"\"\" super ( CNN , self ) . __init__ () # Initialize embeddings self . embeddings = nn . Embedding ( embedding_dim = embedding_dim , num_embeddings = vocab_size , padding_idx = padding_idx , ) # Conv weights self . filter_sizes = filter_sizes self . conv = nn . ModuleList ( [ nn . Conv1d ( in_channels = embedding_dim , out_channels = num_filters , kernel_size = f , ) for f in filter_sizes ] ) # FC weights self . dropout = nn . Dropout ( dropout_p ) self . fc1 = nn . Linear ( num_filters * len ( filter_sizes ), hidden_dim ) self . fc2 = nn . Linear ( hidden_dim , num_classes ) forward ( self , inputs , channel_first = False ) Forward pass. Parameters: Name Type Description Default inputs List List of inputs (by feature). required channel_first bool Channel dimension is first in inputs. Defaults to False. False Returns: Type Description Tensor Outputs from the model. Source code in tagifai/models.py def forward ( self , inputs : List , channel_first : bool = False ) -> torch . Tensor : \"\"\"Forward pass. Args: inputs (List): List of inputs (by feature). channel_first (bool, optional): Channel dimension is first in inputs. Defaults to False. Returns: Outputs from the model. \"\"\" # Embed ( x_in ,) = inputs x_in = self . embeddings ( x_in ) if not channel_first : x_in = x_in . transpose ( 1 , 2 ) # (N, channels, sequence length) z = [] max_seq_len = x_in . shape [ 2 ] for i , f in enumerate ( self . filter_sizes ): # `SAME` padding padding_left = int ( ( self . conv [ i ] . stride [ 0 ] * ( max_seq_len - 1 ) - max_seq_len + self . filter_sizes [ i ] ) / 2 ) padding_right = int ( math . ceil ( ( self . conv [ i ] . stride [ 0 ] * ( max_seq_len - 1 ) - max_seq_len + self . filter_sizes [ i ] ) / 2 ) ) # Conv _z = self . conv [ i ]( F . pad ( x_in , ( padding_left , padding_right ))) # Pool _z = F . max_pool1d ( _z , _z . size ( 2 )) . squeeze ( 2 ) z . append ( _z ) # Concat outputs z = torch . cat ( z , 1 ) # FC z = self . fc1 ( z ) z = self . dropout ( z ) z = self . fc2 ( z ) return z","title":"Models"},{"location":"tagifai/models/#tagifai.models","text":"","title":"tagifai.models"},{"location":"tagifai/models/#classes","text":"","title":"Classes"},{"location":"tagifai/models/#tagifai.models.CNN","text":"","title":"CNN"},{"location":"tagifai/models/#methods","text":"","title":"Methods"},{"location":"tagifai/predict/","text":"Functions predict ( texts , run_id ) Predict tags for an input text using the best model from the best experiment. Usage: texts = [ \"Transfer learning with BERT.\" ] predict ( texts = texts , run_id = \"264ac530b78c42608e5dea1086bc2c73\" ) [ { \"input_text\": \"Transfer learning with BERT.\", \"preprocessed_text\": \"transfer learning bert\", \"predicted_tags\": [ \"attention\", \"language-modeling\", \"natural-language-processing\", \"transfer-learning\", \"transformers\" ] } ] Note The input argument texts can hold multiple input texts and so the resulting prediction dictionary will have len(texts) items. Parameters: Name Type Description Default texts List List of input text to predict tags for. required run_id str ID of the run to load model artifacts from. required Returns: Type Description Dict Predicted tags for input texts. Source code in tagifai/predict.py def predict ( texts : List , run_id : str ) -> Dict : \"\"\"Predict tags for an input text using the best model from the `best` experiment. Usage: ```python texts = [\"Transfer learning with BERT.\"] predict(texts=texts, run_id=\"264ac530b78c42608e5dea1086bc2c73\") ``` <pre> [ { \"input_text\": \"Transfer learning with BERT.\", \"preprocessed_text\": \"transfer learning bert\", \"predicted_tags\": [ \"attention\", \"language-modeling\", \"natural-language-processing\", \"transfer-learning\", \"transformers\" ] } ] </pre> Note: The input argument `texts` can hold multiple input texts and so the resulting prediction dictionary will have `len(texts)` items. Args: texts (List): List of input text to predict tags for. run_id (str): ID of the run to load model artifacts from. Returns: Predicted tags for input texts. \"\"\" # Load artifacts from run client = mlflow . tracking . MlflowClient () run = mlflow . get_run ( run_id = run_id ) device = torch . device ( \"cpu\" ) with tempfile . TemporaryDirectory () as fp : client . download_artifacts ( run_id = run_id , path = \"\" , dst_path = fp ) args = Namespace ( ** utils . load_dict ( filepath = Path ( config . CONFIG_DIR , \"args.json\" )) ) label_encoder = data . LabelEncoder . load ( fp = Path ( fp , \"label_encoder.json\" ) ) tokenizer = data . Tokenizer . load ( fp = Path ( fp , \"tokenizer.json\" )) model_state = torch . load ( Path ( fp , \"model.pt\" ), map_location = device ) # performance = utils.load_dict(filepath=Path(fp, \"performance.json\")) # Load model args = Namespace ( ** run . data . params ) model = train . initialize_model ( args = args , vocab_size = len ( tokenizer ), num_classes = len ( label_encoder ) ) model . load_state_dict ( model_state ) # Prepare data preprocessed_texts = [ data . preprocess ( text ) for text in texts ] X = np . array ( tokenizer . texts_to_sequences ( preprocessed_texts )) y_filler = label_encoder . encode ( [ np . array ([ label_encoder . classes [ 0 ]] * len ( X ))] ) dataset = data . CNNTextDataset ( X = X , y = y_filler , max_filter_size = int ( args . max_filter_size ) ) dataloader = dataset . create_dataloader ( batch_size = int ( args . batch_size )) # Get predictions trainer = train . Trainer ( model = model , device = device ) _ , y_prob = trainer . predict_step ( dataloader ) y_pred = np . array ( [ np . where ( prob >= float ( args . threshold ), 1 , 0 ) for prob in y_prob ] ) tags = label_encoder . decode ( y_pred ) predictions = [ { \"input_text\" : texts [ i ], \"preprocessed_text\" : preprocessed_texts [ i ], \"predicted_tags\" : tags [ i ], } for i in range ( len ( tags )) ] return predictions","title":"Inference"},{"location":"tagifai/predict/#tagifai.predict","text":"","title":"tagifai.predict"},{"location":"tagifai/predict/#functions","text":"","title":"Functions"},{"location":"tagifai/predict/#tagifai.predict.predict","text":"Predict tags for an input text using the best model from the best experiment. Usage: texts = [ \"Transfer learning with BERT.\" ] predict ( texts = texts , run_id = \"264ac530b78c42608e5dea1086bc2c73\" ) [ { \"input_text\": \"Transfer learning with BERT.\", \"preprocessed_text\": \"transfer learning bert\", \"predicted_tags\": [ \"attention\", \"language-modeling\", \"natural-language-processing\", \"transfer-learning\", \"transformers\" ] } ] Note The input argument texts can hold multiple input texts and so the resulting prediction dictionary will have len(texts) items. Parameters: Name Type Description Default texts List List of input text to predict tags for. required run_id str ID of the run to load model artifacts from. required Returns: Type Description Dict Predicted tags for input texts. Source code in tagifai/predict.py def predict ( texts : List , run_id : str ) -> Dict : \"\"\"Predict tags for an input text using the best model from the `best` experiment. Usage: ```python texts = [\"Transfer learning with BERT.\"] predict(texts=texts, run_id=\"264ac530b78c42608e5dea1086bc2c73\") ``` <pre> [ { \"input_text\": \"Transfer learning with BERT.\", \"preprocessed_text\": \"transfer learning bert\", \"predicted_tags\": [ \"attention\", \"language-modeling\", \"natural-language-processing\", \"transfer-learning\", \"transformers\" ] } ] </pre> Note: The input argument `texts` can hold multiple input texts and so the resulting prediction dictionary will have `len(texts)` items. Args: texts (List): List of input text to predict tags for. run_id (str): ID of the run to load model artifacts from. Returns: Predicted tags for input texts. \"\"\" # Load artifacts from run client = mlflow . tracking . MlflowClient () run = mlflow . get_run ( run_id = run_id ) device = torch . device ( \"cpu\" ) with tempfile . TemporaryDirectory () as fp : client . download_artifacts ( run_id = run_id , path = \"\" , dst_path = fp ) args = Namespace ( ** utils . load_dict ( filepath = Path ( config . CONFIG_DIR , \"args.json\" )) ) label_encoder = data . LabelEncoder . load ( fp = Path ( fp , \"label_encoder.json\" ) ) tokenizer = data . Tokenizer . load ( fp = Path ( fp , \"tokenizer.json\" )) model_state = torch . load ( Path ( fp , \"model.pt\" ), map_location = device ) # performance = utils.load_dict(filepath=Path(fp, \"performance.json\")) # Load model args = Namespace ( ** run . data . params ) model = train . initialize_model ( args = args , vocab_size = len ( tokenizer ), num_classes = len ( label_encoder ) ) model . load_state_dict ( model_state ) # Prepare data preprocessed_texts = [ data . preprocess ( text ) for text in texts ] X = np . array ( tokenizer . texts_to_sequences ( preprocessed_texts )) y_filler = label_encoder . encode ( [ np . array ([ label_encoder . classes [ 0 ]] * len ( X ))] ) dataset = data . CNNTextDataset ( X = X , y = y_filler , max_filter_size = int ( args . max_filter_size ) ) dataloader = dataset . create_dataloader ( batch_size = int ( args . batch_size )) # Get predictions trainer = train . Trainer ( model = model , device = device ) _ , y_prob = trainer . predict_step ( dataloader ) y_pred = np . array ( [ np . where ( prob >= float ( args . threshold ), 1 , 0 ) for prob in y_prob ] ) tags = label_encoder . decode ( y_pred ) predictions = [ { \"input_text\" : texts [ i ], \"preprocessed_text\" : preprocessed_texts [ i ], \"predicted_tags\" : tags [ i ], } for i in range ( len ( tags )) ] return predictions","title":"predict()"},{"location":"tagifai/train/","text":"Classes Trainer Object used to facilitate training. Methods eval_step ( self , dataloader ) Evaluation (val / test) step. Parameters: Name Type Description Default dataloader torch.utils.data.DataLoader Torch dataloader to load batches from. required Source code in tagifai/train.py def eval_step ( self , dataloader ): \"\"\"Evaluation (val / test) step. Args: dataloader (torch.utils.data.DataLoader): Torch dataloader to load batches from. \"\"\" # Set model to eval mode self . model . eval () loss = 0.0 y_trues , y_probs = [], [] # Iterate over val batches with torch . no_grad (): for i , batch in enumerate ( dataloader ): # Step batch = [ item . to ( self . device ) for item in batch ] # Set device inputs , y_true = batch [: - 1 ], batch [ - 1 ] z = self . model ( inputs ) # Forward pass J = self . loss_fn ( z , y_true ) . item () # Cumulative Metrics loss += ( J - loss ) / ( i + 1 ) # Store outputs y_prob = torch . sigmoid ( z ) . cpu () . numpy () y_probs . extend ( y_prob ) y_trues . extend ( y_true . cpu () . numpy ()) return loss , np . vstack ( y_trues ), np . vstack ( y_probs ) predict_step ( self , dataloader ) Prediction (inference) step. Note Loss is not calculated for this loop. Parameters: Name Type Description Default dataloader torch.utils.data.dataloader.DataLoader Torch dataloader to load batches from. required Source code in tagifai/train.py def predict_step ( self , dataloader : torch . utils . data . DataLoader ): \"\"\"Prediction (inference) step. Note: Loss is not calculated for this loop. Args: dataloader (torch.utils.data.DataLoader): Torch dataloader to load batches from. \"\"\" # Set model to eval mode self . model . eval () y_trues , y_probs = [], [] # Iterate over val batches with torch . no_grad (): for i , batch in enumerate ( dataloader ): # Forward pass w/ inputs batch = [ item . to ( self . device ) for item in batch ] # Set device inputs , y_true = batch [: - 1 ], batch [ - 1 ] z = self . model ( inputs ) # Store outputs y_prob = torch . sigmoid ( z ) . cpu () . numpy () y_probs . extend ( y_prob ) y_trues . extend ( y_true . cpu () . numpy ()) return np . vstack ( y_trues ), np . vstack ( y_probs ) train ( self , num_epochs , patience , train_dataloader , val_dataloader ) Training loop. Parameters: Name Type Description Default num_epochs int Maximum number of epochs to train for (can stop earlier based on performance). required patience int Number of acceptable epochs for continuous degrading performance. required train_dataloader torch.utils.data.dataloader.DataLoader Dataloader object with training data split. required val_dataloader torch.utils.data.dataloader.DataLoader Dataloader object with validation data split. required Exceptions: Type Description optuna.TrialPruned Early stopping of the optimization trial if poor performance. Returns: Type Description Tuple The best validation loss and the trained model from that point. Source code in tagifai/train.py def train ( self , num_epochs : int , patience : int , train_dataloader : torch . utils . data . DataLoader , val_dataloader : torch . utils . data . DataLoader , ) -> Tuple : \"\"\"Training loop. Args: num_epochs (int): Maximum number of epochs to train for (can stop earlier based on performance). patience (int): Number of acceptable epochs for continuous degrading performance. train_dataloader (torch.utils.data.DataLoader): Dataloader object with training data split. val_dataloader (torch.utils.data.DataLoader): Dataloader object with validation data split. Raises: optuna.TrialPruned: Early stopping of the optimization trial if poor performance. Returns: The best validation loss and the trained model from that point. \"\"\" best_val_loss = np . inf best_model = None _patience = patience for epoch in range ( num_epochs ): # Steps train_loss = self . train_step ( dataloader = train_dataloader ) val_loss , _ , _ = self . eval_step ( dataloader = val_dataloader ) self . scheduler . step ( val_loss ) # Pruning based on the intermediate value if self . trial : self . trial . report ( val_loss , epoch ) if self . trial . should_prune (): logger . info ( \"Unpromising trial pruned!\" ) raise optuna . TrialPruned () # Early stopping if val_loss < best_val_loss : best_val_loss = val_loss best_model = self . model _patience = patience # reset _patience else : _patience -= 1 if not _patience : # 0 logger . info ( \"Stopping early!\" ) break # Logging logger . info ( f \"Epoch: { epoch + 1 } | \" f \"train_loss: { train_loss : .5f } , \" f \"val_loss: { val_loss : .5f } , \" f \"lr: { self . optimizer . param_groups [ 0 ][ 'lr' ] : .2E } , \" f \"_patience: { _patience } \" ) return best_val_loss , best_model train_step ( self , dataloader ) Train step. Parameters: Name Type Description Default dataloader torch.utils.data.DataLoader Torch dataloader to load batches from. required Source code in tagifai/train.py def train_step ( self , dataloader ): \"\"\"Train step. Args: dataloader (torch.utils.data.DataLoader): Torch dataloader to load batches from. \"\"\" # Set model to train mode self . model . train () loss = 0.0 # Iterate over train batches for i , batch in enumerate ( dataloader ): # Step batch = [ item . to ( self . device ) for item in batch ] # Set device inputs , targets = batch [: - 1 ], batch [ - 1 ] self . optimizer . zero_grad () # Reset gradients z = self . model ( inputs ) # Forward pass J = self . loss_fn ( z , targets ) # Define loss J . backward () # Backward pass self . optimizer . step () # Update weights # Cumulative Metrics loss += ( J . detach () . item () - loss ) / ( i + 1 ) return loss Functions evaluate ( dataloader , model , device , threshold , classes ) Evaluate performance on data. Parameters: Name Type Description Default dataloader torch.utils.data.dataloader.DataLoader Dataloader with the data your want to evaluate. required model Module Trained model. required threshold float Precision recall threshold determined during training. required classes List List of unique classes. required device device Device to run model on. Defaults to CPU. required Returns: Type Description Dict Performance metrics. Source code in tagifai/train.py def evaluate ( dataloader : torch . utils . data . DataLoader , model : nn . Module , device : torch . device , threshold : float , classes : List , ) -> Dict : \"\"\"Evaluate performance on data. Args: dataloader (torch.utils.data.DataLoader): Dataloader with the data your want to evaluate. model (nn.Module): Trained model. threshold (float): Precision recall threshold determined during training. classes (List): List of unique classes. device (torch.device): Device to run model on. Defaults to CPU. Returns: Performance metrics. \"\"\" # Determine predictions using threshold trainer = Trainer ( model = model , device = device ) y_true , y_prob = trainer . predict_step ( dataloader = dataloader ) y_pred = np . array ([ np . where ( prob >= threshold , 1 , 0 ) for prob in y_prob ]) # Evaluate performance = get_performance ( y_true = y_true , y_pred = y_pred , classes = classes ) return performance find_best_threshold ( y_true , y_prob ) Determine the best threshold for maximum f1 score. Usage: # Find best threshold _ , y_true , y_prob = trainer . eval_step ( dataloader = train_dataloader ) args . threshold = find_best_threshold ( y_true = y_true , y_prob = y_prob ) Parameters: Name Type Description Default y_true ndarray True labels. required y_prob ndarray Probability distribution for predicted labels. required Returns: Type Description float Best threshold for maximum f1 score. Source code in tagifai/train.py def find_best_threshold ( y_true : np . ndarray , y_prob : np . ndarray ) -> float : \"\"\"Determine the best threshold for maximum f1 score. Usage: ```python # Find best threshold _, y_true, y_prob = trainer.eval_step(dataloader=train_dataloader) args.threshold = find_best_threshold(y_true=y_true, y_prob=y_prob) ``` Args: y_true (np.ndarray): True labels. y_prob (np.ndarray): Probability distribution for predicted labels. Returns: Best threshold for maximum f1 score. \"\"\" precisions , recalls , thresholds = precision_recall_curve ( y_true . ravel (), y_prob . ravel () ) f1s = ( 2 * precisions * recalls ) / ( precisions + recalls ) return thresholds [ np . argmax ( f1s )] get_performance ( y_true , y_pred , classes ) Per-class performance metrics. Parameters: Name Type Description Default y_true ndarray True class labels. required y_pred ndarray Predicted class labels. required classes List List of all unique classes. required Returns: Type Description Dict Dictionary of overall and per-class performance metrics. Source code in tagifai/train.py def get_performance ( y_true : np . ndarray , y_pred : np . ndarray , classes : List ) -> Dict : \"\"\"Per-class performance metrics. Args: y_true (np.ndarray): True class labels. y_pred (np.ndarray): Predicted class labels. classes (List): List of all unique classes. Returns: Dictionary of overall and per-class performance metrics. \"\"\" # Get metrics performance = { \"overall\" : {}, \"class\" : {}} metrics = precision_recall_fscore_support ( y_true , y_pred ) # Overall performance performance [ \"overall\" ][ \"precision\" ] = np . mean ( metrics [ 0 ]) performance [ \"overall\" ][ \"recall\" ] = np . mean ( metrics [ 1 ]) performance [ \"overall\" ][ \"f1\" ] = np . mean ( metrics [ 2 ]) performance [ \"overall\" ][ \"num_samples\" ] = np . float64 ( np . sum ( metrics [ 3 ])) # Per-class performance for i in range ( len ( classes )): performance [ \"class\" ][ classes [ i ]] = { \"precision\" : metrics [ 0 ][ i ], \"recall\" : metrics [ 1 ][ i ], \"f1\" : metrics [ 2 ][ i ], \"num_samples\" : np . float64 ( metrics [ 3 ][ i ]), } return performance initialize_model ( args , vocab_size , num_classes , device = device ( type = 'cpu' )) Initialize a model using parameters (converted to appropriate data types). Parameters: Name Type Description Default args Namespace Parameters for data processing and training. required vocab_size int Size of the vocabulary. required num_classes int Number on unique classes. required device device Device to run model on. Defaults to CPU. device(type='cpu') Source code in tagifai/train.py def initialize_model ( args : Namespace , vocab_size : int , num_classes : int , device : torch . device = torch . device ( \"cpu\" ), ) -> nn . Module : \"\"\"Initialize a model using parameters (converted to appropriate data types). Args: args (Namespace): Parameters for data processing and training. vocab_size (int): Size of the vocabulary. num_classes (int): Number on unique classes. device (torch.device): Device to run model on. Defaults to CPU. \"\"\" # Initialize model filter_sizes = list ( range ( 1 , int ( args . max_filter_size ) + 1 )) model = models . CNN ( embedding_dim = int ( args . embedding_dim ), vocab_size = int ( vocab_size ), num_filters = int ( args . num_filters ), filter_sizes = filter_sizes , hidden_dim = int ( args . hidden_dim ), dropout_p = float ( args . dropout_p ), num_classes = int ( num_classes ), ) model = model . to ( device ) return model objective ( args , trial ) Objective function for optimization trials. Parameters: Name Type Description Default args Namespace Input arguments for each trial (see config/args.json ) for argument names. required trial Trial Optuna optimization trial. required Returns: Type Description float F1 score from evaluating the trained model on the test data split. Source code in tagifai/train.py def objective ( args : Namespace , trial : optuna . trial . _trial . Trial ) -> float : \"\"\"Objective function for optimization trials. Args: args (Namespace): Input arguments for each trial (see `config/args.json`) for argument names. trial (optuna.trial._trial.Trial): Optuna optimization trial. Returns: F1 score from evaluating the trained model on the test data split. \"\"\" # Paramters (to tune) args . embedding_dim = trial . suggest_int ( \"embedding_dim\" , 128 , 512 ) args . num_filters = trial . suggest_int ( \"num_filters\" , 128 , 512 ) args . hidden_dim = trial . suggest_int ( \"hidden_dim\" , 128 , 512 ) args . dropout_p = trial . suggest_uniform ( \"dropout_p\" , 0.3 , 0.8 ) args . lr = trial . suggest_loguniform ( \"lr\" , 5e-5 , 5e-4 ) # Train (can move some of these outside for efficiency) logger . info ( f \" \\n Trial { trial . number } :\" ) logger . info ( json . dumps ( trial . params , indent = 2 )) artifacts = run ( args = args , trial = trial ) # Set additional attributes args = artifacts [ \"args\" ] performance = artifacts [ \"performance\" ] logger . info ( json . dumps ( performance [ \"overall\" ], indent = 2 )) trial . set_user_attr ( \"threshold\" , args . threshold ) trial . set_user_attr ( \"precision\" , performance [ \"overall\" ][ \"precision\" ]) trial . set_user_attr ( \"recall\" , performance [ \"overall\" ][ \"recall\" ]) trial . set_user_attr ( \"f1\" , performance [ \"overall\" ][ \"f1\" ]) return performance [ \"overall\" ][ \"f1\" ] run ( args , trial = None ) Operations for training. Set seed Set device Load data Clean data Preprocess data Encode labels Split data Tokenize inputs Create dataloaders Initialize model Train model Evaluate model Parameters: Name Type Description Default args Namespace Input arguments for operations. required trial Trial Optuna optimization trial. Defaults to None. None Returns: Type Description Dict Artifacts to save and load for later. Source code in tagifai/train.py def run ( args : Namespace , trial : optuna . trial . _trial . Trial = None ) -> Dict : \"\"\"Operations for training. 1. Set seed 2. Set device 3. Load data 4. Clean data 5. Preprocess data 6. Encode labels 7. Split data 8. Tokenize inputs 9. Create dataloaders 10. Initialize model 11. Train model 12. Evaluate model Args: args (Namespace): Input arguments for operations. trial (optuna.trial._trial.Trail, optional): Optuna optimization trial. Defaults to None. Returns: Artifacts to save and load for later. \"\"\" # 1. Set seed utils . set_seed ( seed = args . seed ) # 2. Set device device = utils . set_device ( cuda = args . cuda ) # 3. Load data df , projects_dict , tags_dict = data . load ( shuffle = args . shuffle , num_samples = args . num_samples ) # 4. Clean data df , tags_dict , tags_above_frequency = data . clean ( df = df , tags_dict = tags_dict , min_tag_freq = args . min_tag_freq ) # 5. Preprocess data df . text = df . text . apply ( data . preprocess , lower = args . lower , stem = args . stem ) # 6. Encode labels y , class_weights , label_encoder = data . encode_labels ( labels = df . tags ) # 7. Split data utils . set_seed ( seed = args . seed ) # needed for skmultilearn X_train , X_val , X_test , y_train , y_val , y_test = data . split ( X = df . text . to_numpy (), y = y , train_size = args . train_size ) # 8. Tokenize inputs X_train , tokenizer = data . tokenize_text ( X = X_train , char_level = args . char_level ) X_val , _ = data . tokenize_text ( X = X_val , char_level = args . char_level , tokenizer = tokenizer ) X_test , _ = data . tokenize_text ( X = X_test , char_level = args . char_level , tokenizer = tokenizer ) # 9. Create dataloaders train_dataloader = data . get_dataloader ( data = [ X_train , y_train ], max_filter_size = args . max_filter_size , batch_size = args . batch_size , ) val_dataloader = data . get_dataloader ( data = [ X_val , y_val ], max_filter_size = args . max_filter_size , batch_size = args . batch_size , ) test_dataloader = data . get_dataloader ( data = [ X_test , y_test ], max_filter_size = args . max_filter_size , batch_size = args . batch_size , ) # 10. Initialize model model = initialize_model ( args = args , vocab_size = len ( tokenizer ), num_classes = len ( label_encoder ), device = device , ) # 11. Train model logger . info ( f \"Arguments: { json . dumps ( args . __dict__ , indent = 2 , cls = NumpyEncoder ) } \" ) args , model , loss = train ( args = args , train_dataloader = train_dataloader , val_dataloader = val_dataloader , model = model , device = device , class_weights = class_weights , trial = trial , ) # 12. Evaluate model device = torch . device ( \"cpu\" ) performance = evaluate ( dataloader = test_dataloader , model = model . to ( device ), device = device , threshold = args . threshold , classes = label_encoder . classes , ) return { \"args\" : args , \"label_encoder\" : label_encoder , \"tokenizer\" : tokenizer , \"model\" : model , \"loss\" : loss , \"performance\" : performance , } train ( args , train_dataloader , val_dataloader , model , device , class_weights , trial = None ) Train a model. Parameters: Name Type Description Default args Namespace Parameters for data processing and training. required train_dataloader torch.utils.data.dataloader.DataLoader train data loader. required val_dataloader torch.utils.data.dataloader.DataLoader val data loader. required model Module Initialize model to train. required device device Device to run model on. required class_weights Dict Dictionary of class weights. required trial Trial Optuna optimization trial. Defaults to None. None Returns: Type Description Tuple The best trained model, loss and performance metrics. Source code in tagifai/train.py def train ( args : Namespace , train_dataloader : torch . utils . data . DataLoader , val_dataloader : torch . utils . data . DataLoader , model : nn . Module , device : torch . device , class_weights : Dict , trial : optuna . trial . _trial . Trial = None , ) -> Tuple : \"\"\"Train a model. Args: args (Namespace): Parameters for data processing and training. train_dataloader (torch.utils.data.DataLoader): train data loader. val_dataloader (torch.utils.data.DataLoader): val data loader. model (nn.Module): Initialize model to train. device (torch.device): Device to run model on. class_weights (Dict): Dictionary of class weights. trial (optuna.trial._trial.Trail, optional): Optuna optimization trial. Defaults to None. Returns: The best trained model, loss and performance metrics. \"\"\" # Define loss class_weights_tensor = torch . Tensor ( np . array ( list ( class_weights . values ()))) loss_fn = nn . BCEWithLogitsLoss ( weight = class_weights_tensor ) # Define optimizer & scheduler optimizer = torch . optim . Adam ( model . parameters (), lr = args . lr ) scheduler = torch . optim . lr_scheduler . ReduceLROnPlateau ( optimizer , mode = \"min\" , factor = 0.05 , patience = 5 ) # Trainer module trainer = Trainer ( model = model , device = device , loss_fn = loss_fn , optimizer = optimizer , scheduler = scheduler , trial = trial , ) # Train best_val_loss , best_model = trainer . train ( args . num_epochs , args . patience , train_dataloader , val_dataloader ) # Find best threshold _ , y_true , y_prob = trainer . eval_step ( dataloader = train_dataloader ) args . threshold = find_best_threshold ( y_true = y_true , y_prob = y_prob ) return args , best_model , best_val_loss","title":"Training"},{"location":"tagifai/train/#tagifai.train","text":"","title":"tagifai.train"},{"location":"tagifai/train/#classes","text":"","title":"Classes"},{"location":"tagifai/train/#tagifai.train.Trainer","text":"Object used to facilitate training.","title":"Trainer"},{"location":"tagifai/train/#methods","text":"","title":"Methods"},{"location":"tagifai/train/#functions","text":"","title":"Functions"},{"location":"tagifai/train/#tagifai.train.evaluate","text":"Evaluate performance on data. Parameters: Name Type Description Default dataloader torch.utils.data.dataloader.DataLoader Dataloader with the data your want to evaluate. required model Module Trained model. required threshold float Precision recall threshold determined during training. required classes List List of unique classes. required device device Device to run model on. Defaults to CPU. required Returns: Type Description Dict Performance metrics. Source code in tagifai/train.py def evaluate ( dataloader : torch . utils . data . DataLoader , model : nn . Module , device : torch . device , threshold : float , classes : List , ) -> Dict : \"\"\"Evaluate performance on data. Args: dataloader (torch.utils.data.DataLoader): Dataloader with the data your want to evaluate. model (nn.Module): Trained model. threshold (float): Precision recall threshold determined during training. classes (List): List of unique classes. device (torch.device): Device to run model on. Defaults to CPU. Returns: Performance metrics. \"\"\" # Determine predictions using threshold trainer = Trainer ( model = model , device = device ) y_true , y_prob = trainer . predict_step ( dataloader = dataloader ) y_pred = np . array ([ np . where ( prob >= threshold , 1 , 0 ) for prob in y_prob ]) # Evaluate performance = get_performance ( y_true = y_true , y_pred = y_pred , classes = classes ) return performance","title":"evaluate()"},{"location":"tagifai/train/#tagifai.train.find_best_threshold","text":"Determine the best threshold for maximum f1 score. Usage: # Find best threshold _ , y_true , y_prob = trainer . eval_step ( dataloader = train_dataloader ) args . threshold = find_best_threshold ( y_true = y_true , y_prob = y_prob ) Parameters: Name Type Description Default y_true ndarray True labels. required y_prob ndarray Probability distribution for predicted labels. required Returns: Type Description float Best threshold for maximum f1 score. Source code in tagifai/train.py def find_best_threshold ( y_true : np . ndarray , y_prob : np . ndarray ) -> float : \"\"\"Determine the best threshold for maximum f1 score. Usage: ```python # Find best threshold _, y_true, y_prob = trainer.eval_step(dataloader=train_dataloader) args.threshold = find_best_threshold(y_true=y_true, y_prob=y_prob) ``` Args: y_true (np.ndarray): True labels. y_prob (np.ndarray): Probability distribution for predicted labels. Returns: Best threshold for maximum f1 score. \"\"\" precisions , recalls , thresholds = precision_recall_curve ( y_true . ravel (), y_prob . ravel () ) f1s = ( 2 * precisions * recalls ) / ( precisions + recalls ) return thresholds [ np . argmax ( f1s )]","title":"find_best_threshold()"},{"location":"tagifai/train/#tagifai.train.get_performance","text":"Per-class performance metrics. Parameters: Name Type Description Default y_true ndarray True class labels. required y_pred ndarray Predicted class labels. required classes List List of all unique classes. required Returns: Type Description Dict Dictionary of overall and per-class performance metrics. Source code in tagifai/train.py def get_performance ( y_true : np . ndarray , y_pred : np . ndarray , classes : List ) -> Dict : \"\"\"Per-class performance metrics. Args: y_true (np.ndarray): True class labels. y_pred (np.ndarray): Predicted class labels. classes (List): List of all unique classes. Returns: Dictionary of overall and per-class performance metrics. \"\"\" # Get metrics performance = { \"overall\" : {}, \"class\" : {}} metrics = precision_recall_fscore_support ( y_true , y_pred ) # Overall performance performance [ \"overall\" ][ \"precision\" ] = np . mean ( metrics [ 0 ]) performance [ \"overall\" ][ \"recall\" ] = np . mean ( metrics [ 1 ]) performance [ \"overall\" ][ \"f1\" ] = np . mean ( metrics [ 2 ]) performance [ \"overall\" ][ \"num_samples\" ] = np . float64 ( np . sum ( metrics [ 3 ])) # Per-class performance for i in range ( len ( classes )): performance [ \"class\" ][ classes [ i ]] = { \"precision\" : metrics [ 0 ][ i ], \"recall\" : metrics [ 1 ][ i ], \"f1\" : metrics [ 2 ][ i ], \"num_samples\" : np . float64 ( metrics [ 3 ][ i ]), } return performance","title":"get_performance()"},{"location":"tagifai/train/#tagifai.train.initialize_model","text":"Initialize a model using parameters (converted to appropriate data types). Parameters: Name Type Description Default args Namespace Parameters for data processing and training. required vocab_size int Size of the vocabulary. required num_classes int Number on unique classes. required device device Device to run model on. Defaults to CPU. device(type='cpu') Source code in tagifai/train.py def initialize_model ( args : Namespace , vocab_size : int , num_classes : int , device : torch . device = torch . device ( \"cpu\" ), ) -> nn . Module : \"\"\"Initialize a model using parameters (converted to appropriate data types). Args: args (Namespace): Parameters for data processing and training. vocab_size (int): Size of the vocabulary. num_classes (int): Number on unique classes. device (torch.device): Device to run model on. Defaults to CPU. \"\"\" # Initialize model filter_sizes = list ( range ( 1 , int ( args . max_filter_size ) + 1 )) model = models . CNN ( embedding_dim = int ( args . embedding_dim ), vocab_size = int ( vocab_size ), num_filters = int ( args . num_filters ), filter_sizes = filter_sizes , hidden_dim = int ( args . hidden_dim ), dropout_p = float ( args . dropout_p ), num_classes = int ( num_classes ), ) model = model . to ( device ) return model","title":"initialize_model()"},{"location":"tagifai/train/#tagifai.train.objective","text":"Objective function for optimization trials. Parameters: Name Type Description Default args Namespace Input arguments for each trial (see config/args.json ) for argument names. required trial Trial Optuna optimization trial. required Returns: Type Description float F1 score from evaluating the trained model on the test data split. Source code in tagifai/train.py def objective ( args : Namespace , trial : optuna . trial . _trial . Trial ) -> float : \"\"\"Objective function for optimization trials. Args: args (Namespace): Input arguments for each trial (see `config/args.json`) for argument names. trial (optuna.trial._trial.Trial): Optuna optimization trial. Returns: F1 score from evaluating the trained model on the test data split. \"\"\" # Paramters (to tune) args . embedding_dim = trial . suggest_int ( \"embedding_dim\" , 128 , 512 ) args . num_filters = trial . suggest_int ( \"num_filters\" , 128 , 512 ) args . hidden_dim = trial . suggest_int ( \"hidden_dim\" , 128 , 512 ) args . dropout_p = trial . suggest_uniform ( \"dropout_p\" , 0.3 , 0.8 ) args . lr = trial . suggest_loguniform ( \"lr\" , 5e-5 , 5e-4 ) # Train (can move some of these outside for efficiency) logger . info ( f \" \\n Trial { trial . number } :\" ) logger . info ( json . dumps ( trial . params , indent = 2 )) artifacts = run ( args = args , trial = trial ) # Set additional attributes args = artifacts [ \"args\" ] performance = artifacts [ \"performance\" ] logger . info ( json . dumps ( performance [ \"overall\" ], indent = 2 )) trial . set_user_attr ( \"threshold\" , args . threshold ) trial . set_user_attr ( \"precision\" , performance [ \"overall\" ][ \"precision\" ]) trial . set_user_attr ( \"recall\" , performance [ \"overall\" ][ \"recall\" ]) trial . set_user_attr ( \"f1\" , performance [ \"overall\" ][ \"f1\" ]) return performance [ \"overall\" ][ \"f1\" ]","title":"objective()"},{"location":"tagifai/train/#tagifai.train.run","text":"Operations for training. Set seed Set device Load data Clean data Preprocess data Encode labels Split data Tokenize inputs Create dataloaders Initialize model Train model Evaluate model Parameters: Name Type Description Default args Namespace Input arguments for operations. required trial Trial Optuna optimization trial. Defaults to None. None Returns: Type Description Dict Artifacts to save and load for later. Source code in tagifai/train.py def run ( args : Namespace , trial : optuna . trial . _trial . Trial = None ) -> Dict : \"\"\"Operations for training. 1. Set seed 2. Set device 3. Load data 4. Clean data 5. Preprocess data 6. Encode labels 7. Split data 8. Tokenize inputs 9. Create dataloaders 10. Initialize model 11. Train model 12. Evaluate model Args: args (Namespace): Input arguments for operations. trial (optuna.trial._trial.Trail, optional): Optuna optimization trial. Defaults to None. Returns: Artifacts to save and load for later. \"\"\" # 1. Set seed utils . set_seed ( seed = args . seed ) # 2. Set device device = utils . set_device ( cuda = args . cuda ) # 3. Load data df , projects_dict , tags_dict = data . load ( shuffle = args . shuffle , num_samples = args . num_samples ) # 4. Clean data df , tags_dict , tags_above_frequency = data . clean ( df = df , tags_dict = tags_dict , min_tag_freq = args . min_tag_freq ) # 5. Preprocess data df . text = df . text . apply ( data . preprocess , lower = args . lower , stem = args . stem ) # 6. Encode labels y , class_weights , label_encoder = data . encode_labels ( labels = df . tags ) # 7. Split data utils . set_seed ( seed = args . seed ) # needed for skmultilearn X_train , X_val , X_test , y_train , y_val , y_test = data . split ( X = df . text . to_numpy (), y = y , train_size = args . train_size ) # 8. Tokenize inputs X_train , tokenizer = data . tokenize_text ( X = X_train , char_level = args . char_level ) X_val , _ = data . tokenize_text ( X = X_val , char_level = args . char_level , tokenizer = tokenizer ) X_test , _ = data . tokenize_text ( X = X_test , char_level = args . char_level , tokenizer = tokenizer ) # 9. Create dataloaders train_dataloader = data . get_dataloader ( data = [ X_train , y_train ], max_filter_size = args . max_filter_size , batch_size = args . batch_size , ) val_dataloader = data . get_dataloader ( data = [ X_val , y_val ], max_filter_size = args . max_filter_size , batch_size = args . batch_size , ) test_dataloader = data . get_dataloader ( data = [ X_test , y_test ], max_filter_size = args . max_filter_size , batch_size = args . batch_size , ) # 10. Initialize model model = initialize_model ( args = args , vocab_size = len ( tokenizer ), num_classes = len ( label_encoder ), device = device , ) # 11. Train model logger . info ( f \"Arguments: { json . dumps ( args . __dict__ , indent = 2 , cls = NumpyEncoder ) } \" ) args , model , loss = train ( args = args , train_dataloader = train_dataloader , val_dataloader = val_dataloader , model = model , device = device , class_weights = class_weights , trial = trial , ) # 12. Evaluate model device = torch . device ( \"cpu\" ) performance = evaluate ( dataloader = test_dataloader , model = model . to ( device ), device = device , threshold = args . threshold , classes = label_encoder . classes , ) return { \"args\" : args , \"label_encoder\" : label_encoder , \"tokenizer\" : tokenizer , \"model\" : model , \"loss\" : loss , \"performance\" : performance , }","title":"run()"},{"location":"tagifai/train/#tagifai.train.train","text":"Train a model. Parameters: Name Type Description Default args Namespace Parameters for data processing and training. required train_dataloader torch.utils.data.dataloader.DataLoader train data loader. required val_dataloader torch.utils.data.dataloader.DataLoader val data loader. required model Module Initialize model to train. required device device Device to run model on. required class_weights Dict Dictionary of class weights. required trial Trial Optuna optimization trial. Defaults to None. None Returns: Type Description Tuple The best trained model, loss and performance metrics. Source code in tagifai/train.py def train ( args : Namespace , train_dataloader : torch . utils . data . DataLoader , val_dataloader : torch . utils . data . DataLoader , model : nn . Module , device : torch . device , class_weights : Dict , trial : optuna . trial . _trial . Trial = None , ) -> Tuple : \"\"\"Train a model. Args: args (Namespace): Parameters for data processing and training. train_dataloader (torch.utils.data.DataLoader): train data loader. val_dataloader (torch.utils.data.DataLoader): val data loader. model (nn.Module): Initialize model to train. device (torch.device): Device to run model on. class_weights (Dict): Dictionary of class weights. trial (optuna.trial._trial.Trail, optional): Optuna optimization trial. Defaults to None. Returns: The best trained model, loss and performance metrics. \"\"\" # Define loss class_weights_tensor = torch . Tensor ( np . array ( list ( class_weights . values ()))) loss_fn = nn . BCEWithLogitsLoss ( weight = class_weights_tensor ) # Define optimizer & scheduler optimizer = torch . optim . Adam ( model . parameters (), lr = args . lr ) scheduler = torch . optim . lr_scheduler . ReduceLROnPlateau ( optimizer , mode = \"min\" , factor = 0.05 , patience = 5 ) # Trainer module trainer = Trainer ( model = model , device = device , loss_fn = loss_fn , optimizer = optimizer , scheduler = scheduler , trial = trial , ) # Train best_val_loss , best_model = trainer . train ( args . num_epochs , args . patience , train_dataloader , val_dataloader ) # Find best threshold _ , y_true , y_prob = trainer . eval_step ( dataloader = train_dataloader ) args . threshold = find_best_threshold ( y_true = y_true , y_prob = y_prob ) return args , best_model , best_val_loss","title":"train()"},{"location":"tagifai/utils/","text":"Functions create_dirs ( dirpath ) Creates a directory from a specified path. Parameters: Name Type Description Default dirpath str full path of the directory to create. required Source code in tagifai/utils.py def create_dirs ( dirpath : str ) -> None : \"\"\"Creates a directory from a specified path. Args: dirpath (str): full path of the directory to create. \"\"\" Path ( dirpath ) . mkdir ( exist_ok = True ) load_dict ( filepath ) Load a dictionary from a JSON's filepath. Parameters: Name Type Description Default filepath str JSON's filepath. required Returns: Type Description Dict A dictionary with the data loaded. Source code in tagifai/utils.py def load_dict ( filepath : str ) -> Dict : \"\"\"Load a dictionary from a JSON's filepath. Args: filepath (str): JSON's filepath. Returns: A dictionary with the data loaded. \"\"\" with open ( filepath , \"r\" ) as fp : d = json . load ( fp ) return d load_json_from_url ( url ) Load JSON data from a URL. Parameters: Name Type Description Default url str URL of the data source. required Returns: Type Description Dict A dictionary with the loaded JSON data. Source code in tagifai/utils.py def load_json_from_url ( url : str ) -> Dict : \"\"\"Load JSON data from a URL. Args: url (str): URL of the data source. Returns: A dictionary with the loaded JSON data. \"\"\" data = json . loads ( urlopen ( url ) . read ()) return data save_dict ( d , filepath ) Save a dictionary to a specific location. Warning This will overwrite any existing file at filepath . Parameters: Name Type Description Default d Dict dictionary to save. required filepath str location to save the dictionary to as a JSON file. required Source code in tagifai/utils.py def save_dict ( d : Dict , filepath : str ) -> None : \"\"\"Save a dictionary to a specific location. Warning: This will overwrite any existing file at `filepath`. Args: d (Dict): dictionary to save. filepath (str): location to save the dictionary to as a JSON file. \"\"\" with open ( filepath , \"w\" ) as fp : json . dump ( d , indent = 2 , sort_keys = False , fp = fp ) set_device ( cuda ) Set the device for computation. Parameters: Name Type Description Default cuda bool Determine whether to use GPU or not (if available). required Returns: Type Description device Device that will be use for compute. Source code in tagifai/utils.py def set_device ( cuda : bool ) -> torch . device : \"\"\"Set the device for computation. Args: cuda (bool): Determine whether to use GPU or not (if available). Returns: Device that will be use for compute. \"\"\" device = torch . device ( \"cuda\" if ( torch . cuda . is_available () and cuda ) else \"cpu\" ) torch . set_default_tensor_type ( \"torch.FloatTensor\" ) if device . type == \"cuda\" : torch . set_default_tensor_type ( \"torch.cuda.FloatTensor\" ) return device set_seed ( seed = 1234 ) Set seed for reproducability. Parameters: Name Type Description Default seed int number to use as the seed. Defaults to 1234. 1234 Source code in tagifai/utils.py def set_seed ( seed : int = 1234 ) -> None : \"\"\"Set seed for reproducability. Args: seed (int, optional): number to use as the seed. Defaults to 1234. \"\"\" # Set seeds np . random . seed ( seed ) random . seed ( seed ) torch . manual_seed ( seed ) torch . cuda . manual_seed ( seed ) torch . cuda . manual_seed_all ( seed ) # multi-GPU","title":"Utilities"},{"location":"tagifai/utils/#tagifai.utils","text":"","title":"tagifai.utils"},{"location":"tagifai/utils/#functions","text":"","title":"Functions"},{"location":"tagifai/utils/#tagifai.utils.create_dirs","text":"Creates a directory from a specified path. Parameters: Name Type Description Default dirpath str full path of the directory to create. required Source code in tagifai/utils.py def create_dirs ( dirpath : str ) -> None : \"\"\"Creates a directory from a specified path. Args: dirpath (str): full path of the directory to create. \"\"\" Path ( dirpath ) . mkdir ( exist_ok = True )","title":"create_dirs()"},{"location":"tagifai/utils/#tagifai.utils.load_dict","text":"Load a dictionary from a JSON's filepath. Parameters: Name Type Description Default filepath str JSON's filepath. required Returns: Type Description Dict A dictionary with the data loaded. Source code in tagifai/utils.py def load_dict ( filepath : str ) -> Dict : \"\"\"Load a dictionary from a JSON's filepath. Args: filepath (str): JSON's filepath. Returns: A dictionary with the data loaded. \"\"\" with open ( filepath , \"r\" ) as fp : d = json . load ( fp ) return d","title":"load_dict()"},{"location":"tagifai/utils/#tagifai.utils.load_json_from_url","text":"Load JSON data from a URL. Parameters: Name Type Description Default url str URL of the data source. required Returns: Type Description Dict A dictionary with the loaded JSON data. Source code in tagifai/utils.py def load_json_from_url ( url : str ) -> Dict : \"\"\"Load JSON data from a URL. Args: url (str): URL of the data source. Returns: A dictionary with the loaded JSON data. \"\"\" data = json . loads ( urlopen ( url ) . read ()) return data","title":"load_json_from_url()"},{"location":"tagifai/utils/#tagifai.utils.save_dict","text":"Save a dictionary to a specific location. Warning This will overwrite any existing file at filepath . Parameters: Name Type Description Default d Dict dictionary to save. required filepath str location to save the dictionary to as a JSON file. required Source code in tagifai/utils.py def save_dict ( d : Dict , filepath : str ) -> None : \"\"\"Save a dictionary to a specific location. Warning: This will overwrite any existing file at `filepath`. Args: d (Dict): dictionary to save. filepath (str): location to save the dictionary to as a JSON file. \"\"\" with open ( filepath , \"w\" ) as fp : json . dump ( d , indent = 2 , sort_keys = False , fp = fp )","title":"save_dict()"},{"location":"tagifai/utils/#tagifai.utils.set_device","text":"Set the device for computation. Parameters: Name Type Description Default cuda bool Determine whether to use GPU or not (if available). required Returns: Type Description device Device that will be use for compute. Source code in tagifai/utils.py def set_device ( cuda : bool ) -> torch . device : \"\"\"Set the device for computation. Args: cuda (bool): Determine whether to use GPU or not (if available). Returns: Device that will be use for compute. \"\"\" device = torch . device ( \"cuda\" if ( torch . cuda . is_available () and cuda ) else \"cpu\" ) torch . set_default_tensor_type ( \"torch.FloatTensor\" ) if device . type == \"cuda\" : torch . set_default_tensor_type ( \"torch.cuda.FloatTensor\" ) return device","title":"set_device()"},{"location":"tagifai/utils/#tagifai.utils.set_seed","text":"Set seed for reproducability. Parameters: Name Type Description Default seed int number to use as the seed. Defaults to 1234. 1234 Source code in tagifai/utils.py def set_seed ( seed : int = 1234 ) -> None : \"\"\"Set seed for reproducability. Args: seed (int, optional): number to use as the seed. Defaults to 1234. \"\"\" # Set seeds np . random . seed ( seed ) random . seed ( seed ) torch . manual_seed ( seed ) torch . cuda . manual_seed ( seed ) torch . cuda . manual_seed_all ( seed ) # multi-GPU","title":"set_seed()"}]}